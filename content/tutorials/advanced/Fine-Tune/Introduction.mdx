---
title: 微调简介
---

import { Tab, Tabs } from 'fumadocs-ui/components/tabs'
import { CallOut } from 'components-docs/call-out/call-out.tsx'

## 为什么要微调 RWKV 模型？

目前开源发布的 RWKV 模型均为基底模型（base model ，又称预训练模型），基底模型在自然语言处理等领域的大规模数据集上进行了训练，具备较强的泛化能力和丰富的知识储备。

但为了保持泛化能力和通用性，RWKV 基底模型并未针对某一类任务作优化。因此，RWKV 模型在某些特定任务上的表现可能不够理想。

而对 RWKV 模型进行微调，通俗地说，是**使用特定领域（如法律、文学、医学等）或任务（材料总结、小说续写等）的高质量数据集对 RWKV 模型进行再次训练**。微调过的 RWKV 模型在对应任务的表现会更高质量且稳定。

相比于从头训练一个全新的模型，微调只需要调整预训练模型的参数就能达到满意的任务效果，需要的训练周期和计算资源更少。

综上所述，我们可以通过微调 RWKV 模型优化其在各种任务中的表现，从而快速构建基于 RWKV 模型的应用场景和落地应用。

## 我需要为微调训练准备什么？

要微调 RWKV 模型，你需要准备**一个 Linux 系统**、基础的 Linux 知识储备，和一张**性能较强的 NVIDIA 显卡**（例如 RTX 4090）。

其次，你需要为 Linux 系统配置训练 RWKV 模型需要的工具，例如 [**NVIDIA CUDA Toolkit**](./FT-Environment#cudatoolkit)、[**conda 虚拟环境**](./FT-Environment#conda)、[**PyTorch 等软件包**](./FT-Environment#install-deps)。

最后，你需要准备用于微调训练的[**数据集**](./FT-Dataset)。

## 消费级显卡可以微调什么模型？[#vram-of-fine-tuning]

<CallOut type="info">
以下是 RWKV-PEFT 各类微调方法搭配不同训练精度的显存需求，我们的测试基于 24GB 显存的 4090 显卡。
</CallOut>

### RWKV-7 模型微调显存需求[#vram-of-rwkv-7-fine-tuning]

<Tabs items={["State tuning", "LoRA 微调", "DiSHA 微调", "PiSSA 微调"]}>
<Tab>
**RWKV-7 模型进行 [State tuning](./RWKV-PEFT/State-Tuning.mdx) 的显存需求：**

| 模型参数 | bf16  | int8 量化 | nf4 量化 |
|------------|---------|--------|--------|
| RWKV7-0.1B | 2.6GB GPU   | 2.4GB GPU  | 2.5GB GPU  | 
| RWKV7-0.4B | 3.1GB GPU  | 2.9GB GPU  | 2.8GB GPU  | 
| RWKV7-1.5B | 5.3GB GPU  | 4.1GB GPU  | 3.7GB GPU  | 
| RWKV7-2.9B   | 8.2GB GPU  | 5.7GB GPU  | 4.7GB GPU  | 
</Tab>
<Tab>
**RWKV-7 模型进行 [LoRA 微调](./RWKV-PEFT/LoRA-Fine-Tuning.mdx) 的显存需求：**

| 模型参数 | bf16  | int8 量化 | nf4 量化 |
|------------|---------|--------|--------|
| RWKV7-0.1B | 2.7GB GPU   | 2.5GB GPU  | 2.4GB GPU  | 
| RWKV7-0.4B | 3.4GB GPU  | 2.9GB GPU  | 2.7GB GPU  | 
| RWKV7-1.5B | 5.6GB GPU  | 4.6GB GPU  | 3.9GB GPU  | 
| RWKV7-2.9B   | 8.8GB GPU  | 6.7GB GPU  | 5.7GB GPU  | 
</Tab>
<Tab>
**RWKV-7 模型进行 [DiSHA 微调](./RWKV-PEFT/DiSHA-Fine-Tuning.mdx) 的显存需求：**

| 模型参数 | bf16  | int8 量化 | nf4 量化 |
|------------|---------|--------|--------|
| RWKV7-0.1B | 2.7GB GPU   | 2.5GB GPU  | 2.4GB GPU  | 
| RWKV7-0.4B | 3.1GB GPU  | 2.9GB GPU  | 2.7GB GPU  | 
| RWKV7-1.5B | 5.6GB GPU  | 4.5GB GPU  | 3.9GB GPU  | 
| RWKV7-2.9B   | 8.8GB GPU  | 6.7GB GPU  | 5.7GB GPU  | 
</Tab>
<Tab>
**RWKV-7 模型进行 [PiSSA 微调](./RWKV-PEFT/Pissa-Fine-Tuning.mdx) 的显存需求：**

| 模型参数 | bf16  | int8 量化 | nf4 量化 |
|------------|---------|--------|--------|
| RWKV7-0.1B | 2.6GB GPU   | 2.5GB GPU  | 2.4GB GPU  | 
| RWKV7-0.4B | 3.4GB GPU  | 3.0GB GPU  | 2.7GB GPU  | 
| RWKV7-1.5B | 5.6GB GPU  | 4.6GB GPU  | 3.9GB GPU  | 
| RWKV7-2.9B   | 8.8GB GPU  | 6.7GB GPU  | 5.7GB GPU  | 
</Tab>
</Tabs>

### RWKV-6 模型微调显存需求[#vram-of-rwkv-6-fine-tuning]

RWKV-6 模型微调的显存需求比 RWKV-7 略高，以下显存需求仅供参考：

|   模型尺寸         | 全参微调 | DiSHA/LoRA/PISSA  | QLoRA/QPissa | State tuning |
| --------- | ---- | ---- | ---- | ---- |
| RWKV6-1.6B | 爆显存 | 7.4GB GPU | 5.6GB GPU | 6.4GB GPU |
| RWKV6-3B | 爆显存  | 12.1GB GPU | 8.2GB GPU | 9.4GB GPU |
| RWKV6-7B | 爆显存  | 23.7GB GPU(bsz 8 爆显存) | 14.9GB GPU(bsz 8 需要 19.5GB) | 18.1GB GPU |