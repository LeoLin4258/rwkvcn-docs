---
title: 基准测试数据
description: RWKV评测数据介绍了RWKV在各种大语言模型基准测试的表现，包括Uncheatable Eval、MMLU等基准测试。
keywords: [RWKV评测, rwkv跑分, RWKV基准测试, RWKV的benchmark]
---

import { Tab, Tabs } from 'fumadocs-ui/components/tabs'
import { RadarChartComponent } from 'components-docs/radar-charts'
import { HeatMap } from 'components-docs/heat-map'
import { CallOut } from 'components-docs/call-out/call-out.tsx'

## Uncheatable Eval 测试[#uncheatable-eval]

<CallOut type="info">
[Uncheatable Eval](https://huggingface.co/spaces/Jellyfish042/UncheatableEval) 是“无法作弊的评测”，它使用最新的论文和新闻文章等实时数据，评估开源大语言模型的真实建模能力和泛化能力。 
</CallOut>

<CallOut type="warning">
Uncheatable Eval 测试的结果是压缩率，因此其评分越低，意味着模型性能越好。
</CallOut>

以下是 RWKV 和其他模型的 Uncheatable Eval 评分对比：

### 14B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tab> 
    <HeatMap name="Uncheatable-Eval-14B"/> 
  </Tab>
  <Tab> 
    <CallOut type="warning">
    为了更直观地观察模型在每项评测上的表现，我们对原始数据进行了归一化处理。
    </CallOut>
    <RadarChartComponent name="Uncheatable-Eval-14B"/> 
  </Tab>
  <Tab> 
    | Name                  | Params (B) | Average (lower=better) | ao3 english | bbc news | wikipedia english | arxiv computer science | arxiv physics | github cpp | github python |
    |----------------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
    | Mistral-Nemo-Base-2407| 12.248| 7.107 | 10.07 | 8.081 | 7.954 | 7.419 | 7.656 | 4.203 | 4.368 |
    | RWKV-6-14B-v2.1       | 14.069| 7.609 | 10.188| 8.518 | 8.343 | 7.916 | 8.04  | 4.93  | 5.33  |
    | Llama-2-13b-hf        | 13.016| 7.676 | 10.524| 8.279 | 8.187 | 8.075 | 8.311 | 4.929 | 5.426 |
    | Qwen1.5-14B           | 14.167| 7.697 | 10.88 | 8.884 | 9.102 | 7.752 | 7.862 | 4.665 | 4.736 |
    | pythia-12b-v0         | 11.846| 8.356 | 11.285| 9.19  | 9.527 | 8.535 | 8.398 | 5.43  | 6.125 |
  </Tab>
</Tabs>

### 7B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tab> 
    <HeatMap name="Uncheatable-Eval-7B"/> 
  </Tab>
  <Tab> 
    <CallOut type="warning">
    为了更直观地观察模型在每项评测上的表现，我们抽取了平均分排行前七的模型，并对原始数据进行了归一化处理。
    </CallOut>
    <RadarChartComponent name="Uncheatable-Eval-7B"/> 
  </Tab>
  <Tab> 
| Name                                     | Params (B) | Average (lower=better) | ao3 ​english | bbc ​news | wikipedia ​english | arxiv ​computer ​science | arxiv ​physics | github ​cpp | github ​python |
|------------------------------------------|------------|------------------------|-------------|----------|-------------------|--------------------------|---------------|------------|---------------|
| Qwen3-8B-Base                            | 8.191      | 7.091                  | 10.890      | 8.718    | 8.255             | 7.207                    | 7.465         | 3.617      | 3.482         |
| Meta-Llama-3-8B                          | 8.030      | 7.162                  | 10.619      | 8.295    | 7.785             | 7.536                    | 7.541         | 4.174      | 4.181         |
| RWKV7-G0-7.2B-20250722-ctx4096           | 7.199      | 7.276                  | 10.187      | 8.490    | 8.073             | 7.505                    | 7.787         | 4.414      | 4.475         |
| Qwen2.5-7B                               | 7.616      | 7.323                  | 11.079      | 8.729    | 8.449             | 7.539                    | 7.792         | 3.868      | 3.806         |
| Mistral-7B-v0.1                          | 7.242      | 7.406                  | 10.662      | 8.306    | 7.976             | 7.745                    | 7.903         | 4.612      | 4.635         |
| falcon-mamba-7b                          | 7.273      | 7.548                  | 10.760      | 8.958    | 8.589             | 7.674                    | 7.737         | 4.437      | 4.680         |
| Zamba2-7B                                | 7.357      | 7.582                  | 10.702      | 8.627    | 8.074             | 7.843                    | 8.124         | 4.833      | 4.869         |
| Minitron-8B-Base                         | 8.272      | 7.582                  | 10.835      | 8.654    | 8.284             | 7.856                    | 8.230         | 4.508      | 4.708         |
| RWKV-x060-World-7B-v3-20241112-ctx4096   | 7.636      | 7.633                  | 10.629      | 8.753    | 8.288             | 7.936                    | 8.109         | 4.786      | 4.929         |
| Yi-1.5-6B                                | 6.061      | 7.689                  | 11.072      | 8.803    | 8.464             | 8.004                    | 8.223         | 4.648      | 4.609         |
| RWKV-x060-World-7B-v2.1-20240507-ctx4096 | 7.636      | 7.697                  | 10.560      | 8.752    | 8.313             | 8.033                    | 8.074         | 5.021      | 5.127         |
| RWKV-5-World-7B-v2-20240128-ctx4096      | 7.518      | 7.786                  | 10.652      | 8.881    | 8.408             | 8.131                    | 8.138         | 5.091      | 5.198         |
| OLMo-2-1124-7B                           | 7.299      | 7.792                  | 10.748      | 8.600    | 8.300             | 7.937                    | 8.124         | 5.464      | 5.371         |
| Llama-2-7b-hf                            | 6.738      | 7.830                  | 10.985      | 8.531    | 8.203             | 8.261                    | 8.447         | 5.098      | 5.286         |
| Phi-3-small-8k-instruct                  | 7.392      | 7.834                  | 11.361      | 8.864    | 8.341             | 8.010                    | 8.488         | 5.187      | 4.588         |
  </Tab>
</Tabs>

### 3B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tab> 
    <HeatMap name="Uncheatable-Eval-3B"/> 
  </Tab>
  <Tab> 
    <CallOut type="warning">
    为了更直观地观察模型在每项评测上的表现，我们抽取了平均分排行前七的模型，并对原始数据进行了归一化处理。
    </CallOut>
    <RadarChartComponent name="Uncheatable-Eval-3B"/> 
  </Tab>
  <Tab> 
    | Name                                     | Params (B)  | Average (lower=better) | ao3 ​english  | bbc ​news | wikipedia ​english  | arxiv ​computer ​science   | arxiv ​physics  | github ​cpp  | github ​python  |
    |------------------------------------------|-------------|------------------------|--------------|----------|--------------------|--------------------------|----------------|-------------|----------------|
    | RWKV7-G1-2.9B-20250519-ctx4096           | 2.948       | 7.566                  | 10.334       | 8.837    | 8.479              | 7.736                    | 8.079          | 4.639       | 4.855          |
    | Llama-3.2-3B                             | 3.213       | 7.597                  | 11.041       | 8.691    | 8.394              | 7.835                    | 7.988          | 4.638       | 4.592          |
    | Qwen2.5-3B                               | 3.086       | 7.681                  | 11.406       | 9.125    | 8.920              | 7.822                    | 8.127          | 4.233       | 4.134          |
    | RWKV-x070-World-2.9B-v3-20250211-ctx4096 | 2.948       | 7.754                  | 10.641       | 8.910    | 8.548              | 7.938                    | 8.220          | 4.939       | 5.085          |
    | stablelm-3b-4e1t                         | 2.795       | 7.856                  | 11.041       | 8.804    | 8.454              | 8.197                    | 8.384          | 4.897       | 5.217          |
    | Falcon-H1-3B-Base                        | 3.149       | 7.890                  | 11.500       | 9.149    | 8.886              | 7.810                    | 8.080          | 4.852       | 4.956          |
    | recurrentgemma-2b                        | 2.683       | 8.000                  | 11.475       | 8.935    | 8.837              | 8.292                    | 8.404          | 4.877       | 5.179          |
    | Phi-3-mini-4k-instruct                   | 3.821       | 8.081                  | 11.959       | 9.173    | 8.671              | 8.129                    | 8.707          | 5.254       | 4.673          |
    | RWKV-x060-World-3B-v2.1-20240417-ctx4096 | 3.100       | 8.089                  | 10.824       | 9.150    | 8.835              | 8.333                    | 8.463          | 5.456       | 5.565          |
    | mamba2attn-2.7b                          | 2.698       | 8.144                  | 11.254       | 9.238    | 8.944              | 8.386                    | 8.170          | 5.268       | 5.745          |
    | Phi-3.5-mini-instruct                    | 3.821       | 8.217                  | 12.138       | 9.281    | 8.814              | 8.310                    | 8.836          | 5.312       | 4.826          |
    | gemma-2b                                 | 2.506       | 8.220                  | 11.775       | 9.158    | 9.065              | 8.546                    | 8.627          | 5.040       | 5.332          |
    | RWKV-5-World-3B-v2-20231113-ctx4096      | 3.063       | 8.234                  | 10.982       | 9.338    | 9.006              | 8.480                    | 8.573          | 5.580       | 5.680          |
    | mamba2-2.7b                              | 2.703       | 8.251                  | 11.351       | 9.327    | 9.040              | 8.481                    | 8.257          | 5.402       | 5.901          |
  </Tab>
</Tabs>

### 1.5B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tab> 
    <HeatMap name="Uncheatable-Eval-1B"/> 
  </Tab>
  <Tab> 
    <CallOut type="warning">
    为了更直观地观察模型在每项评测上的表现，我们抽取了平均分排行前七的模型，并对原始数据进行了归一化处理。
    </CallOut>
    <RadarChartComponent name="Uncheatable-Eval-1B"/> 
  </Tab>
  <Tab> 
  | Name                                      | Params (B) | Average (lower=better) | ao3 ​english | bbc ​news | wikipedia ​english | arxiv ​computer ​science | arxiv ​physics | github ​cpp | github ​python |
  |-------------------------------------------  |------------|------------------------|-------------|----------|-------------------|--------------------------|---------------|------------|---------------|
  | Qwen3-1.7B-Base                             | 1.721      | 7.923                  | 11.846      | 9.728    | 9.366             | 7.844                    | 8.248         | 4.287      | 4.145         |
  | RWKV7-G1-1.5B-20250429-ctx4096              | 1.528      | 7.989                  | 10.797      | 9.263    | 8.955             | 8.099                    | 8.528         | 5.035      | 5.246         |
  | Qwen2.5-1.5B                                | 1.544      | 8.073                  | 11.935      | 9.545    | 9.406             | 8.162                    | 8.537         | 4.533      | 4.396         |
  | RWKV-x070-World-1.5B-v3-20250127-ctx4096    | 1.528      | 8.174                  | 11.098      | 9.317    | 9.001             | 8.298                    | 8.651         | 5.368      | 5.485         |
  | SmolLM2-1.7B                                | 1.711      | 8.231                  | 11.353      | 9.359    | 9.362             | 8.424                    | 8.940         | 5.015      | 5.162         |
  | Llama-3.2-1B                                | 1.236      | 8.249                  | 11.854      | 9.317    | 9.118             | 8.432                    | 8.657         | 5.254      | 5.113         |
  | stablelm-2-1_6b                             | 1.645      | 8.335                  | 11.582      | 9.216    | 8.938             | 8.629                    | 8.980         | 5.557      | 5.444         |
  | Falcon-H1-1.5B-Deep-Base                    | 1.555      | 8.443                  | 11.961      | 9.652    | 9.436             | 8.305                    | 8.859         | 5.476      | 5.409         |
  | RWKV-x060-World-1B6-v2.1-20240328-ctx4096   | 1.600      | 8.499                  | 11.253      | 9.543    | 9.292             | 8.687                    | 8.878         | 5.882      | 5.956         |
  | Falcon-H1-1.5B-Base                         | 1.555      | 8.575                  | 12.098      | 9.786    | 9.602             | 8.397                    | 8.976         | 5.621      | 5.544         |
  | mamba2-1.3b                                 | 1.344      | 8.627                  | 11.768      | 9.696    | 9.450             | 8.805                    | 8.627         | 5.779      | 6.267         |
  | RWKV-5-World-1B5-v2-20231025-ctx4096        | 1.578      | 8.645                  | 11.409      | 9.716    | 9.465             | 8.832                    | 8.989         | 6.012      | 6.092         |
  | mamba-1.4b-hf                               | 1.372      | 8.733                  | 11.847      | 9.774    | 9.535             | 8.953                    | 8.746         | 5.888      | 6.386         |
  | MobileLLM-1.5B                              | 1.562      | 8.737                  | 11.741      | 9.132    | 9.057             | 8.872                    | 9.210         | 6.397      | 6.750         |
  | Zamba2-1.2B                                 | 1.215      | 8.821                  | 11.565      | 9.354    | 9.027             | 8.597                    | 9.103         | 7.260      | 6.840         |
  | OLMo-2-0425-1B                              | 1.485      | 8.826                  | 11.805      | 9.520    | 9.286             | 8.678                    | 9.050         | 6.775      | 6.667         |
  | Qwen1.5-1.8B                                | 1.837      | 8.915                  | 12.507      | 9.885    | 9.985             | 8.965                    | 9.438         | 5.817      | 5.811         |
  | RWKV-4-World-1.5B-v1-fixed-20230612-ctx4096 | 1.578      | 9.005                  | 11.510      | 9.968    | 9.716             | 9.421                    | 9.677         | 6.264      | 6.482         |
  | pythia-1.4b-v0                              | 1.415      | 9.091                  | 12.342      | 10.144   | 9.925             | 9.192                    | 9.081         | 6.224      | 6.731         |
  </Tab>
</Tabs>

## MMLU 测试[#mmlu-eval]

<CallOut type="info">
MMLU 测试（Massive Multitask Language Understanding）是一项用于评估大型语言模型（LLMs）在广泛任务上的多任务语言理解能力的基准测试。

MMLU 涵盖了从初中到研究生水平的57个不同学科，包括数学、物理、历史、法律、生物学等，测试语言模型是否能够在不同领域内进行推理、回答问题和表现出跨学科的知识。
</CallOut>

如果使用 lm_eval 的标准格式测试，RWKV-6-World-7B-v2.1 的 MMLU 准确度是 42.8% ：

``` bash copy
The following are multiple choice questions (with answers) about abstract algebra.
Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.
A. 0
B. 4
C. 2
D. 6

Answer:
```
如果使用 RWKV 的训练的数据格式作为 prompt，RWKV-6-World-7B-v2.1 的 MMLU 是 46.7%：

``` bash copy
User: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.
A. 0
B. 4
C. 2
D. 6

Assistant: The answer is
```

如果使用最适合 RWKV 模型推理的 prompt 模板，RWKV-6-World-7B-v2.1 的 MMLU 是 47.9%：

``` bash copy
User: You are a very talented expert in <SUBJECT>. Answer this question:
<Question>
A. <|A|>
B. <|B|>
C. <|C|>
D. <|D|>

Assistant: The answer is
```

<CallOut type="info">
数据来源：https://github.com/Jellyfish042/rwkv_mmlu
</CallOut>
