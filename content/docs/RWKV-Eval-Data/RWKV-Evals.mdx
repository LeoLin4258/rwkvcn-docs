---
title: 基准测试数据
description: RWKV评测数据介绍了RWKV在各种大语言模型基准测试的表现，包括Uncheatable Eval、MMLU等基准测试。
keywords: [RWKV评测, rwkv跑分, RWKV基准测试, RWKV的benchmark]
---

import { Tab, Tabs } from 'fumadocs-ui/components/tabs'
import { RadarChartComponent } from 'components-docs/radar-charts'
import { HeatMap } from 'components-docs/heat-map'
import { CallOut } from 'components-docs/call-out/call-out.tsx'
import { LineChart } from 'components-docs/line-chart'

## Uncheatable Eval 测试[#uncheatable-eval]

<CallOut type="info">
[Uncheatable Eval](https://huggingface.co/spaces/Jellyfish042/UncheatableEval) 是“无法作弊的评测”，它使用最新的论文和新闻文章等实时数据，评估开源大语言模型的真实建模能力和泛化能力。 
</CallOut>

<CallOut type="warning">
Uncheatable Eval 测试的结果是压缩率，因此其评分越低，意味着模型性能越好。
</CallOut>

以下是 RWKV 和其他模型的 Uncheatable Eval 评分对比：

### 14B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tab> 
    <HeatMap name="Uncheatable-Eval-14B"/> 
  </Tab>
  <Tab> 
    <CallOut type="warning">
    为了更直观地观察模型在每项评测上的表现，我们对原始数据进行了归一化处理。
    </CallOut>
    <RadarChartComponent name="Uncheatable-Eval-14B"/> 
  </Tab>
  <Tab> 
      | Name | Params (B) | Average (lower=better) | ao3 english | bbc news | wikipedia english | arxiv computer science | arxiv physics | github cpp | github python |
      | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
      | Qwen3-14B-Base | 14.768 | 6.845 | 10.569 | 8.445 | 7.942 | 7.001 | 7.210 | 3.439 | 3.312 |
      | rwkv7-g0b-13.3b | 13.269 | 6.870 | 9.848 | 8.202 | 7.636 | 7.108 | 7.380 | 4.026 | 3.892 |
      | gemma-3-12b-pt | 12.187 | 6.945 | 10.540 | 7.914 | 7.607 | 7.286 | 7.387 | 3.883 | 3.997 |
      | Qwen2.5-14B | 14.770 | 6.951 | 10.558 | 8.317 | 7.944 | 7.224 | 7.392 | 3.625 | 3.599 |
      | Mistral-Nemo-Base-2407 | 12.248 | 6.970 | 10.165 | 8.118 | 7.642 | 7.287 | 7.455 | 4.079 | 4.042 |
      | Motif-2-12.7B-Base | 12.704 | 7.099 | 10.628 | 8.328 | 7.897 | 7.134 | 7.404 | 4.189 | 4.114 |
      | Llama-2-13b-hf | 13.016 | 7.540 | 10.655 | 8.307 | 7.901 | 7.993 | 8.122 | 4.795 | 5.009 |
  </Tab>
</Tabs>

### 7B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tab> 
    <HeatMap name="Uncheatable-Eval-7B"/> 
  </Tab>
  <Tab> 
    <CallOut type="warning">
    为了更直观地观察模型在每项评测上的表现，我们抽取了平均分排行前七的模型，并对原始数据进行了归一化处理。
    </CallOut>
    <RadarChartComponent name="Uncheatable-Eval-7B"/> 
  </Tab>
  <Tab> 
    | Name | Params (B) | Average (lower=better) | ao3 english | bbc news | wikipedia english | arxiv computer science | arxiv physics | github cpp | github python |
    | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
    | Qwen3-8B-Base | 8.191 | 7.091 | 10.890 | 8.718 | 8.255 | 7.207 | 7.465 | 3.617 | 3.482 |
    | Meta-Llama-3-8B | 8.030 | 7.162 | 10.619 | 8.295 | 7.785 | 7.536 | 7.541 | 4.174 | 4.181 |
    | RWKV7-g0a3-7.2b-20251029-ctx8192 | 7.199 | 7.222 | 10.164 | 8.480 | 7.996 | 7.440 | 7.747 | 4.378 | 4.347 |
    | Qwen2.5-7B | 7.616 | 7.323 | 11.079 | 8.729 | 8.449 | 7.539 | 7.792 | 3.868 | 3.806 |
    | Falcon-H1-7B-Base | 7.586 | 7.339 | 10.958 | 8.576 | 8.225 | 7.403 | 7.569 | 4.251 | 4.392 |
    | Mistral-7B-v0.1 | 7.242 | 7.406 | 10.662 | 8.306 | 7.976 | 7.745 | 7.903 | 4.612 | 4.635 |
    | Hunyuan-7B-Pretrain | 7.505 | 7.541 | 11.509 | 8.987 | 8.499 | 7.653 | 8.108 | 4.201 | 3.829 |
    | falcon-mamba-7b | 7.273 | 7.548 | 10.760 | 8.958 | 8.589 | 7.674 | 7.737 | 4.437 | 4.680 |
    | Zamba2-7B | 7.357 | 7.582 | 10.702 | 8.627 | 8.074 | 7.843 | 8.124 | 4.833 | 4.869 |
    | Minitron-8B-Base | 8.272 | 7.582 | 10.835 | 8.654 | 8.284 | 7.856 | 8.230 | 4.508 | 4.708 |
    | Olmo-3-1025-7B | 7.298 | 7.595 | 11.101 | 8.784 | 8.522 | 7.490 | 7.947 | 4.930 | 4.394 |
    | RWKV-x060-World-7B-v3-20241112-ctx4096 | 7.636 | 7.633 | 10.629 | 8.753 | 8.288 | 7.936 | 8.109 | 4.786 | 4.929 |
  </Tab>
</Tabs>

### 3B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tab> 
    <HeatMap name="Uncheatable-Eval-3B"/> 
  </Tab>
  <Tab> 
    <CallOut type="warning">
    为了更直观地观察模型在每项评测上的表现，我们抽取了平均分排行前七的模型，并对原始数据进行了归一化处理。
    </CallOut>
    <RadarChartComponent name="Uncheatable-Eval-3B"/> 
  </Tab>
  <Tab> 
    | Name | Params (B) | Average (lower=better) | ao3 english | bbc news | wikipedia english | arxiv computer science | arxiv physics | github cpp | github python |
    | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
    | RWKV7-g1a4-2.9b-20251118-ctx8192 | 2.948 | 7.486 | 10.481 | 8.800 | 8.310 | 7.712 | 8.072 | 4.553 | 4.474 |
    | Llama-3.2-3B | 3.213 | 7.643 | 11.219 | 8.701 | 8.365 | 7.928 | 8.065 | 4.661 | 4.562 |
    | Qwen2.5-3B | 3.086 | 7.722 | 11.575 | 9.139 | 8.895 | 7.911 | 8.220 | 4.203 | 4.113 |
    | SmolLM3-3B-Base | 3.075 | 7.784 | 11.187 | 8.905 | 8.611 | 8.097 | 8.631 | 4.513 | 4.546 |
    | RWKV-x070-World-2.9B-v3-20250211-ctx4096 | 2.948 | 7.800 | 10.812 | 8.909 | 8.501 | 8.049 | 8.307 | 4.955 | 5.066 |
    | stablelm-3b-4e1t | 2.795 | 7.907 | 11.211 | 8.815 | 8.434 | 8.299 | 8.476 | 4.906 | 5.207 |
    | Falcon-H1-3B-Base | 3.149 | 7.936 | 11.685 | 9.158 | 8.910 | 7.891 | 8.161 | 4.832 | 4.917 |
    | recurrentgemma-2b | 2.683 | 8.052 | 11.632 | 8.951 | 8.835 | 8.401 | 8.488 | 4.897 | 5.157 |
    | RWKV-x060-World-3B-v2.1-20240417-ctx4096 | 3.100 | 8.147 | 11.005 | 9.161 | 8.815 | 8.451 | 8.559 | 5.479 | 5.561 |
    | mamba2attn-2.7b | 2.698 | 8.204 | 11.436 | 9.246 | 8.947 | 8.474 | 8.236 | 5.336 | 5.751 |  
    </Tab>
</Tabs>

### 1.5B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tab> 
    <HeatMap name="Uncheatable-Eval-1B"/> 
  </Tab>
  <Tab> 
    <CallOut type="warning">
    为了更直观地观察模型在每项评测上的表现，我们抽取了平均分排行前七的模型，并对原始数据进行了归一化处理。
    </CallOut>
    <RadarChartComponent name="Uncheatable-Eval-1B"/> 
  </Tab>
  <Tab> 
    | Name | Params (B) | Average (lower=better) | ao3 english | bbc news | wikipedia english | arxiv computer science | arxiv physics | github cpp | github python |
    | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
    | Qwen3-1.7B-Base | 1.721 | 7.965 | 12.016 | 9.743 | 9.352 | 7.936 | 8.350 | 4.260 | 4.095 |
    | rwkv7-g1b-1.5b-20251015-ctx8192 | 1.527 | 7.969 | 10.972 | 9.250 | 8.843 | 8.110 | 8.537 | 5.041 | 5.027 |
    | Qwen2.5-1.5B | 1.544 | 8.124 | 12.114 | 9.562 | 9.393 | 8.270 | 8.646 | 4.502 | 4.384 |
    | RWKV-x070-World-1.5B-v3-20250127-ctx4096 | 1.527 | 8.231 | 11.273 | 9.320 | 8.965 | 8.431 | 8.758 | 5.385 | 5.483 |
    | SmolLM2-1.7B | 1.711 | 8.298 | 11.536 | 9.373 | 9.351 | 8.547 | 9.047 | 5.080 | 5.152 |
    | Llama-3.2-1B | 1.236 | 8.306 | 12.036 | 9.331 | 9.097 | 8.556 | 8.755 | 5.267 | 5.101 |
    | Index-1.9B | 2.173 | 8.340 | 11.831 | 9.493 | 9.069 | 8.497 | 8.561 | 5.380 | 5.547 |
    | stablelm-2-1_6b | 1.645 | 8.396 | 11.761 | 9.237 | 8.943 | 8.762 | 9.088 | 5.558 | 5.425 |
    | Falcon-H1-1.5B-Deep-Base | 1.555 | 8.505 | 12.144 | 9.666 | 9.482 | 8.407 | 8.968 | 5.497 | 5.368 |
    | RWKV-x060-World-1B6-v2.1-20240328-ctx4096 | 1.600 | 8.564 | 11.434 | 9.555 | 9.276 | 8.822 | 8.990 | 5.906 | 5.968 |
    | Falcon-H1-1.5B-Base | 1.555 | 8.639 | 12.287 | 9.796 | 9.645 | 8.507 | 9.089 | 5.635 | 5.514 |
    | mamba2-1.3b | 1.344 | 8.699 | 11.944 | 9.710 | 9.463 | 8.925 | 8.714 | 5.851 | 6.286 |
    | RWKV-5-World-1B5-v2-20231025-ctx4096 | 1.578 | 8.715 | 11.595 | 9.731 | 9.451 | 8.977 | 9.103 | 6.039 | 6.110 |
    | mamba-1.4b-hf | 1.372 | 8.806 | 12.026 | 9.783 | 9.552 | 9.081 | 8.836 | 5.958 | 6.408 |  
    </Tab>
</Tabs>

## MMLU 测试[#mmlu-eval]
<CallOut type="info">
MMLU 测试（Massive Multitask Language Understanding）评估模型的多任务语言理解能力的基准测试。MMLU 涵盖了从初中到研究生水平的 57 个不同学科，包括数学、物理、历史、法律、生物学等，测试语言模型是否能够在不同领域内进行推理、回答问题和表现出跨学科的知识。
</CallOut>

<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-MMLU-Benchmark"  seriesFilter="0,2"/>
  </Tab>
  <Tab> 
    | Model | MMLU | MMLU COT |
    | --- | --- | --- |
    | **rwkv7-g0b-13.3b** | 0.765 | 0.827 |
    | **rwkv7-g0b-7.2b** | 0.660 | 0.731 |
    | **rwkv7-g1b-2.9b** | 0.622 | 0.669 |
    | **rwkv7-g1b-1.5b** | 0.505 | 0.542 |
   </Tab>
</Tabs>

## MMLU Pro 测试[#mmlu_pro-eval]
<CallOut type="info">
MMLU-Pro 是一个更稳健且更具挑战性的大规模多任务理解数据集，更严格地评测大型语言模型的能力。该数据集包含涵盖各个学科的 1.2 万个复杂问题。
</CallOut>
<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-MMLU-PRO-Benchmark" seriesFilter="0,2"/>
  </Tab>
  <Tab> 
    | Model | MMLU-PRO | MMLU-PRO COT |
    | --- | --- | --- |
    | **rwkv7-g0b-13.3b** | 0.502 | 0.612 |
    | **rwkv7-g0b-7.2b** | 0.361 | 0.526 |
    | **rwkv7-g1b-2.9b** | 0.322 | 0.433 |
    | **rwkv7-g1b-1.5b** | 0.222 | 0.292 |
   </Tab>
</Tabs>

## MMLU 变体测试[#mmlu-variants]
<CallOut type="info">
MMLU Redux 是 MMLU 的精简和修正版本，移除了部分错误标签；MMMLU (Multilingual MMLU) 则是多语言版本的测试。
</CallOut>
<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-MMLU-Variants-Benchmark" seriesFilter="0,4"/>
  </Tab>
  <Tab> 
    | Model | MMLU Redux | Redux COT | MMMLU | MMMLU COT |
    | --- | --- | --- | --- | --- |
    | **rwkv7-g0b-13.3b** | 0.787 | 0.863 | 0.765 | 0.827 |
    | **rwkv7-g0b-7.2b** | 0.685 | 0.772 | 0.660 | 0.731 |
    | **rwkv7-g1b-2.9b** | 0.641 | 0.699 | 0.622 | 0.669 |
    | **rwkv7-g1b-1.5b** | 0.529 | 0.569 | 0.505 | 0.541 |
   </Tab>
</Tabs>

## GSM8K 测试[#gsm8k]
<CallOut type="info">
GSM8K (Grade School Math 8K) 是一个包含 8500 道高质量、语言表达多样的小学数学应用题数据集，用于评估模型的数学推理能力。
</CallOut>
<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-GSM8K-Benchmark" seriesFilter="0,1"/>
  </Tab>
  <Tab> 
    | Model | GSM8K COT |
    | --- | --- |
    | **rwkv7-g0b-13.3b** | 0.923 |
    | **rwkv7-g0b-7.2b** | 0.851 |
    | **rwkv7-g1b-2.9b** | 0.766 |
    | **rwkv7-g1b-1.5b** | 0.585 |
   </Tab>
</Tabs>

## MATH500 测试[#math500-eval]
<CallOut type="info">
MATH-500 是衡量 AI 模型数学推理能力的权威基准测试，包含 500 道具有挑战性的数学问题，涵盖代数、几何、微积分、概率统计等多个领域。
</CallOut>

<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-MATH500-Benchmark" seriesFilter="0,1"/>
  </Tab>
  <Tab> 
    | Model | MATH500 COT |
    | --- | --- |
    | **rwkv7-g0b-13.3b** | 0.768 |
    | **rwkv7-g0b-7.2b** | 0.635 |
    | **rwkv7-g1b-2.9b** | 0.495 |
    | **rwkv7-g1b-1.5b** | 0.298 |
   </Tab>
</Tabs>

## 通用数学与推理测试[#general-math]
<CallOut type="info">
包含 Hendrycks Math, SVAMP, ASDiv, MAWPS 等数据集，以及 Algebra 222 和 Math Odyssey。涵盖了从基础算术、代数到几何等多种类型的数学问题，重点考察模型的思维链（CoT）能力。
</CallOut>
<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-GeneralMath-Benchmark" seriesFilter="0,6"/>
  </Tab>
  <Tab> 
    | Model | Hendrycks Math | SVAMP | ASDiv | MAWPS |
    | --- | --- | --- | --- | --- |
    | **rwkv7-g0b-13.3b** | 0.558 | 0.942 | 0.923 | 0.949 |
    | **rwkv7-g0b-7.2b** | 0.488 | 0.926 | 0.905 | 0.924 |
    | **rwkv7-g1b-2.9b** | 0.383 | 0.840 | 0.844 | 0.890 |
    | **rwkv7-g1b-1.5b** | 0.246 | 0.683 | 0.735 | 0.813 |

    | Model | GSM+ | Algebra 222 | Math Odyssey |
    | --- | --- | --- | --- |
    | **rwkv7-g0b-13.3b** | 0.767 | 0.892 | 0.494 |
    | **rwkv7-g0b-7.2b** | 0.685 | 0.856 | 0.401 |
    | **rwkv7-g1b-2.9b** | 0.575 | 0.795 | 0.320 |
    | **rwkv7-g1b-1.5b** | 0.406 | 0.658 | 0.202 |
   </Tab>
</Tabs>

## 代码生成能力测试[#coding-eval]
<CallOut type="info">
包含 HumanEval 和 MBPP (Mostly Basic Python Problems) 及其扩展版本。这些测试评估模型将自然语言描述转换为可执行代码的能力，涵盖了基础编程逻辑到复杂算法实现。
</CallOut>
<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-Coding-Benchmark" seriesFilter="0,4"/>
  </Tab>
  <Tab> 
    | Model | HumanEval | HumanEval+ | MBPP | MBPP+ |
    | --- | --- | --- | --- | --- |
    | **rwkv7-g0b-13.3b** | 0.817 | 0.762 | 0.820 | 0.706 |
    | **rwkv7-g0b-7.2b** | 0.640 | 0.604 | 0.757 | 0.640 |
    | **rwkv7-g1b-2.9b** | 0.537 | 0.494 | 0.627 | 0.550 |
    | **rwkv7-g1b-1.5b** | 0.396 | 0.348 | 0.439 | 0.368 |
   </Tab>
</Tabs>
<CallOut type="warning">
HumanEval Fix / CN 补充数据：
*   **rwkv7-g0b-13.3b**: Fix 0.823 / CN 0.799
*   **rwkv7-g0b-7.2b**: Fix 0.585 / CN 0.659
*   **rwkv7-g1b-2.9b**: Fix 0.524 / CN 0.524
*   **rwkv7-g1b-1.5b**: Fix 0.323 / CN 0.390
</CallOut>

## 中文综合能力测试[#chinese-eval]
<CallOut type="info">
C-Eval 和 CMMLU 是针对中文大模型的综合性能力评测基准，涵盖了人文、社科、理工等多个学科，考察模型在中文语境下的知识和推理能力。
</CallOut>
<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-Chinese-Benchmark" seriesFilter="0,4"/>
  </Tab>
  <Tab> 
    | Model | C-Eval | C-Eval COT | CMMLU | CMMLU COT |
    | --- | --- | --- | --- | --- |
    | **rwkv7-g0b-13.3b** | 0.640 | 0.674 | 0.667 | 0.689 |
    | **rwkv7-g0b-7.2b** | 0.540 | 0.563 | 0.564 | 0.598 |
    | **rwkv7-g1b-2.9b** | 0.496 | 0.513 | 0.523 | 0.556 |
    | **rwkv7-g1b-1.5b** | 0.427 | 0.426 | 0.422 | 0.442 |
   </Tab>
</Tabs>
<CallOut type="warning">
**Gaokao 2023 English COT** (高考英语):
13.3b: 0.665 | 7.2b: 0.535 | 2.9b: 0.447 | 1.5b: 0.273
</CallOut>

## IFEval [#ifeval-eval]
<CallOut type="info">
IFEval（Instruction-Following Evaluation）是一个专门用于评估大模型指令跟随能力的基准数据集。核心特点是使用**可验证指令（verifiable instructions）**，通过确定性规则（如正则匹配、计数、格式检查）自动判断模型是否遵循指令。
</CallOut>
<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-IFEval-Benchmark" seriesFilter="0,1"/>
  </Tab>
  <Tab> 
    | Model | IFEval (strict prompt-level) |
    | --- | --- |
    | **rwkv7-g0b-13.3b** | 0.689 |
    | **rwkv7-g0b-7.2b** | 0.579 |
    | **rwkv7-g1b-2.9b** | 0.494 |
    | **rwkv7-g1b-1.5b** | 0.421 |
   </Tab>
</Tabs>

## GPQA 测试[#gpqa-eval]
<CallOut type="info">
GPQA (Graduate-Level Google-Proof Q&A Benchmark) 是一个极具挑战性的问答数据集，包含生物、物理和化学领域的研究生水平问题。这些问题设计为"Google-Proof"，即通过简单的搜索引擎查询很难找到直接答案。
</CallOut>
<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-GPQA-Benchmark" seriesFilter="0,2"/>
  </Tab>
  <Tab> 
    | Model | GPQA Main | GPQA Main COT | SuperGPQA |
    | --- | --- | --- | --- |
    | **rwkv7-g0b-13.3b** | 0.379 | 0.429 | 0.288 |
    | **rwkv7-g0b-7.2b** | 0.308 | 0.317 | 0.220 |
    | **rwkv7-g1b-2.9b** | 0.333 | 0.326 | 0.194 |
    | **rwkv7-g1b-1.5b** | 0.283 | 0.279 | 0.158 |
   </Tab>
</Tabs>