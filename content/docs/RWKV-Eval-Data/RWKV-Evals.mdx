---
title: 基准测试数据
description: RWKV评测数据介绍了RWKV在各种大语言模型基准测试的表现，包括Uncheatable Eval、MMLU等基准测试。
keywords: [RWKV评测, rwkv跑分, RWKV基准测试, RWKV的benchmark]
---

import { Tab, Tabs } from 'fumadocs-ui/components/tabs'
import { RadarChartComponent } from 'components-docs/radar-charts'
import { HeatMap } from 'components-docs/heat-map'
import { CallOut } from 'components-docs/call-out/call-out.tsx'
import { LineChart } from 'components-docs/line-chart'

## Uncheatable Eval 测试[#uncheatable-eval]

<CallOut type="info">
[Uncheatable Eval](https://huggingface.co/spaces/Jellyfish042/UncheatableEval) 是“无法作弊的评测”，它使用最新的论文和新闻文章等实时数据，评估开源大语言模型的真实建模能力和泛化能力。 
</CallOut>

<CallOut type="warning">
Uncheatable Eval 测试的结果是压缩率，因此其评分越低，意味着模型性能越好。
</CallOut>

以下是 RWKV 和其他模型的 Uncheatable Eval 评分对比：

### 14B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tab> 
    <HeatMap name="Uncheatable-Eval-14B"/> 
  </Tab>
  <Tab> 
    <CallOut type="warning">
    为了更直观地观察模型在每项评测上的表现，我们对原始数据进行了归一化处理。
    </CallOut>
    <RadarChartComponent name="Uncheatable-Eval-14B"/> 
  </Tab>
  <Tab> 
      | Name | Params (B) | Average (lower=better) | ao3 english | bbc news | wikipedia english | arxiv computer science | arxiv physics | github cpp | github python |
      | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
      | Qwen3-14B-Base | 14.768 | 6.845 | 10.569 | 8.445 | 7.942 | 7.001 | 7.210 | 3.439 | 3.312 |
      | RWKV7-g0a4-13.3b-20251114-ctx8192 | 13.269 | 6.870 | 9.848 | 8.202 | 7.636 | 7.108 | 7.380 | 4.026 | 3.892 |
      | gemma-3-12b-pt | 12.187 | 6.945 | 10.540 | 7.914 | 7.607 | 7.286 | 7.387 | 3.883 | 3.997 |
      | Qwen2.5-14B | 14.770 | 6.951 | 10.558 | 8.317 | 7.944 | 7.224 | 7.392 | 3.625 | 3.599 |
      | Mistral-Nemo-Base-2407 | 12.248 | 6.970 | 10.165 | 8.118 | 7.642 | 7.287 | 7.455 | 4.079 | 4.042 |
      | Motif-2-12.7B-Base | 12.704 | 7.099 | 10.628 | 8.328 | 7.897 | 7.134 | 7.404 | 4.189 | 4.114 |
      | Llama-2-13b-hf | 13.016 | 7.540 | 10.655 | 8.307 | 7.901 | 7.993 | 8.122 | 4.795 | 5.009 |
  </Tab>
</Tabs>

### 7B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tab> 
    <HeatMap name="Uncheatable-Eval-7B"/> 
  </Tab>
  <Tab> 
    <CallOut type="warning">
    为了更直观地观察模型在每项评测上的表现，我们抽取了平均分排行前七的模型，并对原始数据进行了归一化处理。
    </CallOut>
    <RadarChartComponent name="Uncheatable-Eval-7B"/> 
  </Tab>
  <Tab> 
    | Name | Params (B) | Average (lower=better) | ao3 english | bbc news | wikipedia english | arxiv computer science | arxiv physics | github cpp | github python |
    | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
    | Qwen3-8B-Base | 8.191 | 7.091 | 10.890 | 8.718 | 8.255 | 7.207 | 7.465 | 3.617 | 3.482 |
    | Meta-Llama-3-8B | 8.030 | 7.162 | 10.619 | 8.295 | 7.785 | 7.536 | 7.541 | 4.174 | 4.181 |
    | RWKV7-g0a3-7.2b-20251029-ctx8192 | 7.199 | 7.222 | 10.164 | 8.480 | 7.996 | 7.440 | 7.747 | 4.378 | 4.347 |
    | Qwen2.5-7B | 7.616 | 7.323 | 11.079 | 8.729 | 8.449 | 7.539 | 7.792 | 3.868 | 3.806 |
    | Falcon-H1-7B-Base | 7.586 | 7.339 | 10.958 | 8.576 | 8.225 | 7.403 | 7.569 | 4.251 | 4.392 |
    | Mistral-7B-v0.1 | 7.242 | 7.406 | 10.662 | 8.306 | 7.976 | 7.745 | 7.903 | 4.612 | 4.635 |
    | Hunyuan-7B-Pretrain | 7.505 | 7.541 | 11.509 | 8.987 | 8.499 | 7.653 | 8.108 | 4.201 | 3.829 |
    | falcon-mamba-7b | 7.273 | 7.548 | 10.760 | 8.958 | 8.589 | 7.674 | 7.737 | 4.437 | 4.680 |
    | Zamba2-7B | 7.357 | 7.582 | 10.702 | 8.627 | 8.074 | 7.843 | 8.124 | 4.833 | 4.869 |
    | Minitron-8B-Base | 8.272 | 7.582 | 10.835 | 8.654 | 8.284 | 7.856 | 8.230 | 4.508 | 4.708 |
    | Olmo-3-1025-7B | 7.298 | 7.595 | 11.101 | 8.784 | 8.522 | 7.490 | 7.947 | 4.930 | 4.394 |
    | RWKV-x060-World-7B-v3-20241112-ctx4096 | 7.636 | 7.633 | 10.629 | 8.753 | 8.288 | 7.936 | 8.109 | 4.786 | 4.929 |
  </Tab>
</Tabs>

### 3B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tab> 
    <HeatMap name="Uncheatable-Eval-3B"/> 
  </Tab>
  <Tab> 
    <CallOut type="warning">
    为了更直观地观察模型在每项评测上的表现，我们抽取了平均分排行前七的模型，并对原始数据进行了归一化处理。
    </CallOut>
    <RadarChartComponent name="Uncheatable-Eval-3B"/> 
  </Tab>
  <Tab> 
    | Name | Params (B) | Average (lower=better) | ao3 english | bbc news | wikipedia english | arxiv computer science | arxiv physics | github cpp | github python |
    | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
    | RWKV7-g1a4-2.9b-20251118-ctx8192 | 2.948 | 7.486 | 10.481 | 8.800 | 8.310 | 7.712 | 8.072 | 4.553 | 4.474 |
    | Llama-3.2-3B | 3.213 | 7.643 | 11.219 | 8.701 | 8.365 | 7.928 | 8.065 | 4.661 | 4.562 |
    | Qwen2.5-3B | 3.086 | 7.722 | 11.575 | 9.139 | 8.895 | 7.911 | 8.220 | 4.203 | 4.113 |
    | SmolLM3-3B-Base | 3.075 | 7.784 | 11.187 | 8.905 | 8.611 | 8.097 | 8.631 | 4.513 | 4.546 |
    | RWKV-x070-World-2.9B-v3-20250211-ctx4096 | 2.948 | 7.800 | 10.812 | 8.909 | 8.501 | 8.049 | 8.307 | 4.955 | 5.066 |
    | stablelm-3b-4e1t | 2.795 | 7.907 | 11.211 | 8.815 | 8.434 | 8.299 | 8.476 | 4.906 | 5.207 |
    | Falcon-H1-3B-Base | 3.149 | 7.936 | 11.685 | 9.158 | 8.910 | 7.891 | 8.161 | 4.832 | 4.917 |
    | recurrentgemma-2b | 2.683 | 8.052 | 11.632 | 8.951 | 8.835 | 8.401 | 8.488 | 4.897 | 5.157 |
    | RWKV-x060-World-3B-v2.1-20240417-ctx4096 | 3.100 | 8.147 | 11.005 | 9.161 | 8.815 | 8.451 | 8.559 | 5.479 | 5.561 |
    | mamba2attn-2.7b | 2.698 | 8.204 | 11.436 | 9.246 | 8.947 | 8.474 | 8.236 | 5.336 | 5.751 |  
    </Tab>
</Tabs>

### 1.5B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tab> 
    <HeatMap name="Uncheatable-Eval-1B"/> 
  </Tab>
  <Tab> 
    <CallOut type="warning">
    为了更直观地观察模型在每项评测上的表现，我们抽取了平均分排行前七的模型，并对原始数据进行了归一化处理。
    </CallOut>
    <RadarChartComponent name="Uncheatable-Eval-1B"/> 
  </Tab>
  <Tab> 
    | Name | Params (B) | Average (lower=better) | ao3 english | bbc news | wikipedia english | arxiv computer science | arxiv physics | github cpp | github python |
    | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
    | Qwen3-1.7B-Base | 1.721 | 7.965 | 12.016 | 9.743 | 9.352 | 7.936 | 8.350 | 4.260 | 4.095 |
    | RWKV7-g1a3-1.5b-20251015-ctx8192 | 1.527 | 7.969 | 10.972 | 9.250 | 8.843 | 8.110 | 8.537 | 5.041 | 5.027 |
    | Qwen2.5-1.5B | 1.544 | 8.124 | 12.114 | 9.562 | 9.393 | 8.270 | 8.646 | 4.502 | 4.384 |
    | RWKV-x070-World-1.5B-v3-20250127-ctx4096 | 1.527 | 8.231 | 11.273 | 9.320 | 8.965 | 8.431 | 8.758 | 5.385 | 5.483 |
    | SmolLM2-1.7B | 1.711 | 8.298 | 11.536 | 9.373 | 9.351 | 8.547 | 9.047 | 5.080 | 5.152 |
    | Llama-3.2-1B | 1.236 | 8.306 | 12.036 | 9.331 | 9.097 | 8.556 | 8.755 | 5.267 | 5.101 |
    | Index-1.9B | 2.173 | 8.340 | 11.831 | 9.493 | 9.069 | 8.497 | 8.561 | 5.380 | 5.547 |
    | stablelm-2-1_6b | 1.645 | 8.396 | 11.761 | 9.237 | 8.943 | 8.762 | 9.088 | 5.558 | 5.425 |
    | Falcon-H1-1.5B-Deep-Base | 1.555 | 8.505 | 12.144 | 9.666 | 9.482 | 8.407 | 8.968 | 5.497 | 5.368 |
    | RWKV-x060-World-1B6-v2.1-20240328-ctx4096 | 1.600 | 8.564 | 11.434 | 9.555 | 9.276 | 8.822 | 8.990 | 5.906 | 5.968 |
    | Falcon-H1-1.5B-Base | 1.555 | 8.639 | 12.287 | 9.796 | 9.645 | 8.507 | 9.089 | 5.635 | 5.514 |
    | mamba2-1.3b | 1.344 | 8.699 | 11.944 | 9.710 | 9.463 | 8.925 | 8.714 | 5.851 | 6.286 |
    | RWKV-5-World-1B5-v2-20231025-ctx4096 | 1.578 | 8.715 | 11.595 | 9.731 | 9.451 | 8.977 | 9.103 | 6.039 | 6.110 |
    | mamba-1.4b-hf | 1.372 | 8.806 | 12.026 | 9.783 | 9.552 | 9.081 | 8.836 | 5.958 | 6.408 |  
    </Tab>
</Tabs>

## MMLU 测试[#mmlu-eval]

<CallOut type="info">
MMLU 测试（Massive Multitask Language Understanding）是一项用于评估大型语言模型（LLMs）在广泛任务上的多任务语言理解能力的基准测试。

MMLU 涵盖了从初中到研究生水平的57个不同学科，包括数学、物理、历史、法律、生物学等，测试语言模型是否能够在不同领域内进行推理、回答问题和表现出跨学科的知识。
</CallOut>

<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-MMLU-Benchmark"  seriesFilter="0,2"/>
  </Tab>
  <Tab> 
    | Model | MMLU | MMLU COT |
    | --- | --- | --- |
    | **rwkv7-g0a4-13.3b-20251114-ctx8192** | 0.762 | 0.821 |
    | **rwkv7-g0a3-7-2b-20251029-ctx8192** | 0.651 | 0.723 |
    | **rwkv7-g1a4-2-9b-20251118-ctx8192** | 0.613 | 0.675 |
    | **rwkv7-g1a3-1-5b-20251015-ctx8192** | 0.502 | 0.541 |
   </Tab>
</Tabs>

## MMLU Pro 测试[#mmlu_pro-eval]
<CallOut type="info">
MMLU-Pro 是一个更稳健且更具挑战性的大规模多任务理解数据集，旨在更严格地评测大型语言模型的能力。该数据集包含涵盖各个学科的 1.2 万个复杂问题。
</CallOut>
<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-MMLU-PRO-Benchmark" seriesFilter="0,2"/>
  </Tab>
  <Tab> 
    | Model | MMLU-PRO | MMLU-PRO COT |
    | --- | --- | --- |
    | **rwkv7-g0a4-13.3b-20251114-ctx8192** | 0.5 | 0.602 |
    | **rwkv7-g0a3-7-2b-20251029-ctx8192** | 0.359 | 0.521 |
    | **rwkv7-g1a4-2-9b-20251118-ctx8192** | 0.324 | 0.43 |
    | **rwkv7-g1a3-1-5b-20251015-ctx8192** | 0.219 | 0.285 |
   </Tab>
</Tabs>

## GSM8K 测试[#gsm8k]
<CallOut type="info">
GSM8K (Grade School Math 8K) 是一个包含 8,500 道高质量、语言表达多样的小学数学应用题数据集，用于评估模型的数学推理能力。
</CallOut>
<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-GSM8K-Benchmark" seriesFilter="0,1"/>
  </Tab>
  <Tab> 
    | Model | GSM8K |
    | --- | --- |
    | **rwkv7-g0a4-13.3b** | 0.912 |
    | **rwkv7-g0a3-7.2b** | 0.839 |
    | **rwkv7-g1a4-2.9b** | 0.773 |
    | **rwkv7-g1a3-1.5b** | 0.591 |
   </Tab>
</Tabs>

## MATH500
<CallOut type="info">
MATH-500是衡量AI模型数学推理能力的权威基准测试，包含500道具有挑战性的数学问题，涵盖代数、几何、微积分、概率统计等多个领域。该测试要求模型不仅能够给出正确答案，还需要展示出清晰、严谨的推理过程，对模型的逻辑思维能力提出了极高要求。
</CallOut>

<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-MATH500-Benchmark" seriesFilter="0,1"/>
  </Tab>
  <Tab> 
    | Model | MATH500 |
    | --- | --- |
    | **rwkv7-g0a4-13.3b** | 0.74 |
    | **rwkv7-g0a3-7.2b** | 0.678 |
    | **rwkv7-g1a4-2.9b** | 0.482 |
    | **rwkv7-g1a3-1.5b** | 0.278 |
   </Tab>
</Tabs>

## IFEval
<CallOut type="info">
IFEval（Instruction-Following Evaluation）是一个专门用于评估 大型语言模型（LLMs）指令跟随能力 的基准数据集。其核心特点是使用**可验证指令（verifiable instructions）**，即可以通过确定性规则（如正则匹配、计数、格式检查）自动判断模型是否遵循指令。
</CallOut>
<Tabs items={[' 条形图 ', ' 原始数据 ']}>
  <Tab> 
      <LineChart name="RWKV-7-IFEval-Benchmark" seriesFilter="0,1"/>
  </Tab>
  <Tab> 
    | Model | IFEval (strict prompt-level) |
    | --- | --- |
    | **rwkv7-g0a4-13.3b** | 0.678 |
    | **rwkv7-g0a3-7.2b** | 0.58 |
    | **rwkv7-g1a3-2.9b** | 0.51 |
    | **rwkv7-g1a3-1.5b** | 0.425 |
   </Tab>
</Tabs>