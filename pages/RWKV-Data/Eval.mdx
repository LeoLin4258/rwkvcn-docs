---
title: RWKV基准测试数据 - RWKV评测数据
description: RWKV评测数据介绍了RWKV在各种大语言模型基准测试的表现，包括Uncheatable Eval、MMLU、LongBench、RULER等基准测试。
keywords: RWKV评测,rwkv跑分,RWKV基准测试,RWKV的benchmark
---

import { Callout } from 'nextra/components'
import { Steps } from 'nextra/components'
import { Tabs } from 'nextra/components'
import { Cards, Card } from 'nextra/components'
import { RadarChartComponent } from '/components/docs/radar-charts.tsx'
import { HeatMap } from '/components/docs/heat-map.tsx'

## Uncheatable Eval 测试[#uncheatable-eval]

<Callout type="info" emoji="ℹ️">
[Uncheatable Eval](https://huggingface.co/spaces/Jellyfish042/UncheatableEval) 是“无法作弊的评测”，它使用最新的论文和新闻文章等实时数据，评估开源大语言模型的真实建模能力和泛化能力。 
</Callout>

<Callout type="warning" emoji="⚠️">
Uncheatable Eval 测试的结果是压缩率，因此其评分越低，意味着模型性能越好。
</Callout>

以下是 RWKV 和其他模型的 Uncheatable Eval 评分对比：

### 14B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tabs.Tab> 
    <HeatMap name="Uncheatable-Eval-14B"/> 
  </Tabs.Tab>
  <Tabs.Tab> 
    <Callout type="warning" emoji="⚠️">
    为了更直观地观察模型在每项评测上的表现，我们对原始数据进行了归一化处理。
    </Callout>
    <RadarChartComponent name="Uncheatable-Eval-14B"/> 
  </Tabs.Tab>
  <Tabs.Tab> 
    | Name                  | Params (B) | Average (lower=better) | ao3 english | bbc news | wikipedia english | arxiv computer science | arxiv physics | github cpp | github python |
    |----------------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|
    | Mistral-Nemo-Base-2407| 12.248| 7.107 | 10.07 | 8.081 | 7.954 | 7.419 | 7.656 | 4.203 | 4.368 |
    | RWKV-6-14B-v2.1       | 14.069| 7.609 | 10.188| 8.518 | 8.343 | 7.916 | 8.04  | 4.93  | 5.33  |
    | Llama-2-13b-hf        | 13.016| 7.676 | 10.524| 8.279 | 8.187 | 8.075 | 8.311 | 4.929 | 5.426 |
    | Qwen1.5-14B           | 14.167| 7.697 | 10.88 | 8.884 | 9.102 | 7.752 | 7.862 | 4.665 | 4.736 |
    | pythia-12b-v0         | 11.846| 8.356 | 11.285| 9.19  | 9.527 | 8.535 | 8.398 | 5.43  | 6.125 |
  </Tabs.Tab>
</Tabs>

### 7B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tabs.Tab> 
    <HeatMap name="Uncheatable-Eval-7B"/> 
  </Tabs.Tab>
  <Tabs.Tab> 
    <Callout type="warning" emoji="⚠️">
    为了更直观地观察模型在每项评测上的表现，我们抽取了平均分排行前七的模型，并对原始数据进行了归一化处理。
    </Callout>
    <RadarChartComponent name="Uncheatable-Eval-7B"/> 
  </Tabs.Tab>
  <Tabs.Tab> 
    | Name                           | Params (B)  | Average (lower=better) | ao3 ​english | bbc ​news | wikipedia ​english | arxiv ​computer ​science | arxiv ​physics | github ​cpp | github ​python |
    |--------------------------------|-------------|------------------------|--------------|----------|--------------------|--------------------------|----------------|-------------|----------------|
    | Meta-Llama-3.1-8B              | 8.030       | 7.227                  | 10.529       | 8.201    | 7.932              | 7.549                    | 7.686          | 4.204       | 4.487          |
    | Qwen2.5-7B                     | 7.616       | 7.453                  | 10.931       | 8.742    | 8.815              | 7.683                    | 7.991          | 3.920       | 4.091          |
    | Qwen2-7B                       | 7.616       | 7.528                  | 10.814       | 8.582    | 8.709              | 7.822                    | 8.249          | 4.204       | 4.317          |
    | Mistral-7B-v0.1                | 7.242       | 7.580                  | 10.546       | 8.282    | 8.435              | 7.852                    | 8.092          | 4.800       | 5.051          |
    | **RWKV-6-World-7B-v2.1**       | 7.636       | 7.817                  | 10.410       | 8.740    | 8.577              | 8.107                    | 8.248          | 5.122       | 5.516          |
    | Yi-1.5-6B                      | 6.061       | 7.830                  | 10.926       | 8.789    | 8.948              | 8.098                    | 8.410          | 4.746       | 4.890          |
    | OLMo-1.7-7B-hf                 | 6.888       | 7.881                  | 11.012       | 8.647    | 8.987              | 8.004                    | 8.200          | 4.966       | 5.354          |
    | **RWKV-5-World-7B-v2**         | 7.518       | 7.905                  | 10.495       | 8.878    | 8.694              | 8.184                    | 8.311          | 5.187       | 5.587          |
    | Qwen1.5-7B                     | 7.721       | 7.916                  | 11.100       | 9.126    | 9.357              | 7.953                    | 8.107          | 4.845       | 4.924          |
    | mpt-7b                         | 6.649       | 7.950                  | 11.185       | 8.682    | 8.771              | 8.157                    | 8.438          | 4.954       | 5.466          |
    | Llama-2-7b-hf                  | 6.738       | 7.967                  | 10.842       | 8.511    | 8.521              | 8.330                    | 8.630          | 5.235       | 5.703          |
    | Zamba-7B-v1                    | 7.232       | 8.089                  | 10.846       | 8.520    | 8.639              | 8.060                    | 8.290          | 5.938       | 6.328          |
    | open_llama_7b_v2               | 6.738       | 8.103                  | 11.086       | 8.839    | 9.053              | 8.404                    | 8.757          | 4.887       | 5.698          |
    | falcon-7b                      | 6.922       | 8.302                  | 10.756       | 8.691    | 9.147              | 8.549                    | 9.060          | 5.763       | 6.149          |
    | pythia-6.9b-v0                 | 6.857       | 8.545                  | 11.495       | 9.375    | 9.757              | 8.683                    | 8.572          | 5.611       | 6.321          |
    | mamba-7b-rw                    | 6.947       | 9.782                  | 10.808       | 8.548    | 8.990              | 8.613                    | 9.142          | 11.107      | 11.268         |
  </Tabs.Tab>
</Tabs>

### 3B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tabs.Tab> 
    <HeatMap name="Uncheatable-Eval-3B"/> 
  </Tabs.Tab>
  <Tabs.Tab> 
    <Callout type="warning" emoji="⚠️">
    为了更直观地观察模型在每项评测上的表现，我们抽取了平均分排行前七的模型，并对原始数据进行了归一化处理。
    </Callout>
    <RadarChartComponent name="Uncheatable-Eval-3B"/> 
  </Tabs.Tab>
  <Tabs.Tab> 
    | Name                                     | Params (B)  | Average (lower=better) | ao3 ​english | bbc ​news | wikipedia ​english | arxiv ​computer ​science | arxiv ​physics | github ​cpp | github ​python |
    |------------------------------------------|-------------|------------------------|--------------|----------|--------------------|--------------------------|----------------|-------------|----------------|
    | Qwen1.5-4B                               | 3.950       | 8.250                  | 11.612       | 9.335    | 9.781              | 8.233                    | 8.449          | 5.143       | 5.197          |
    | Llama-3.1-Minitron-4B-Depth-Base         | 4.540       | 8.257                  | 11.404       | 9.261    | 9.399              | 8.456                    | 9.068          | 4.912       | 5.298          |
    | **RWKV-6-3B-v2.1-20240417-ctx4096**      | 3.100       | 8.263                  | 10.835       | 9.156    | 9.103              | 8.492                    | 8.721          | 5.573       | 5.961          |
    | Phi-3-mini-4k-instruct                   | 3.821       | 8.333                  | 11.978       | 9.190    | 9.301              | 8.386                    | 9.015          | 5.443       | 5.021          |
    | mamba2attn-2.7b                          | 2.698       | 8.359                  | 11.285       | 9.227    | 9.593              | 8.520                    | 8.399          | 5.405       | 6.084          |
    | gemma-2b                                 | 2.506       | 8.385                  | 11.741       | 9.137    | 9.391              | 8.686                    | 8.878          | 5.190       | 5.675          |
    | **RWKV-5-World-3B-v2-20231113-ctx4096**  | 3.063       | 8.409                  | 10.987       | 9.343    | 9.297              | 8.620                    | 8.836          | 5.699       | 6.080          |
    | open_llama_3b_v2                         | 3.426       | 8.459                  | 11.466       | 9.149    | 9.470              | 8.744                    | 9.156          | 5.205       | 6.023          |
    | mamba2-2.7b                              | 2.703       | 8.469                  | 11.377       | 9.316    | 9.717              | 8.617                    | 8.489          | 5.524       | 6.243          |
    | Phi-3.5-mini-instruct                    | 3.821       | 8.476                  | 12.163       | 9.313    | 9.431              | 8.567                    | 9.147          | 5.513       | 5.197          |
    | Zamba2-2.7B                              | 2.689       | 8.571                  | 11.166       | 8.934    | 9.143              | 8.424                    | 8.877          | 6.776       | 6.675          |
    | mamba-2.8b-hf                            | 2.768       | 8.593                  | 11.462       | 9.428    | 9.872              | 8.760                    | 8.636          | 5.638       | 6.352          |
    | **RWKV-4-World-3B-v1**                   | 3.063       | 8.705                  | 11.042       | 9.512    | 9.588              | 9.129                    | 9.425          | 5.854       | 6.383          |
    | pythia-2.8b-v0                           | 2.775       | 8.845                  | 11.815       | 9.684    | 10.155             | 8.923                    | 8.855          | 5.887       | 6.597          |
    | RedPajama-INCITE-Base-3B-v1              | 2.776       | 8.869                  | 11.661       | 9.128    | 9.290              | 8.882                    | 9.208          | 6.617       | 7.294          |
    | phi-2                                    | 2.780       | 8.911                  | 12.280       | 9.283    | 9.582              | 8.815                    | 9.858          | 6.771       | 5.789          |
    | btlm-3b-8k-base                          | 2.646       | 8.959                  | 11.807       | 9.082    | 9.097              | 8.571                    | 8.880          | 7.464       | 7.812          |
    | **RWKV-4-Pile-3B-20221110-ctx4096**      | 2.985       | 9.020                  | 11.790       | 9.758    | 10.397             | 9.204                    | 9.059          | 6.085       | 6.850          |
    | Sheared-LLaMA-2.7B                       | 2.702       | 9.099                  | 11.580       | 9.146    | 9.613              | 9.107                    | 9.647          | 7.066       | 7.535          |
    | MiniCPM3-4B                              | 4.074       | 9.119                  | 13.229       | 10.711   | 10.719             | 8.754                    | 9.034          | 5.545       | 5.839          |
    | mamba-2.8b-slimpj                        | 2.768       | 9.247                  | 13.811       | 9.660    | 9.151              | 8.711                    | 8.922          | 7.043       | 7.428          |
    | OpenELM-3B                               | 3.037       | 9.679                  | 14.051       | 10.080   | 9.974              | 9.159                    | 9.517          | 7.402       | 7.572          |
  </Tabs.Tab>
</Tabs>

### 1.6B 参数模型

<Tabs items={[' 热力图 ', ' 雷达图 ', ' 原始数据 ']}>
  <Tabs.Tab> 
    <HeatMap name="Uncheatable-Eval-1B"/> 
  </Tabs.Tab>
  <Tabs.Tab> 
    <Callout type="warning" emoji="⚠️">
    为了更直观地观察模型在每项评测上的表现，我们抽取了平均分排行前七的模型，并对原始数据进行了归一化处理。
    </Callout>
    <RadarChartComponent name="Uncheatable-Eval-1B"/> 
  </Tabs.Tab>
  <Tabs.Tab> 
  | Name                                     | Params (B)  | Average (lower=better) | ao3 ​english | bbc ​news | wikipedia ​english | arxiv ​computer ​science | arxiv ​physics | github ​cpp | github ​python |
  |------------------------------------------|-------------|------------------------|--------------|----------|--------------------|--------------------------|----------------|-------------|----------------|
  | Qwen2-1.5B                               | 1.544       | 8.457                  | 11.777       | 9.443    | 9.876              | 8.751                    | 9.323          | 5.019       | 5.009          |
  | Index-1.9B                               | 2.173       | 8.491                  | 11.662       | 9.495    | 9.521              | 8.546                    | 8.717          | 5.533       | 5.963          |
  | stablelm-2-1_6b                          | 1.645       | 8.531                  | 11.563       | 9.211    | 9.381              | 8.791                    | 9.248          | 5.646       | 5.875          |
  | Rene-v0.1-1.3b-pytorch                   | 1.330       | 8.559                  | 11.621       | 9.199    | 9.839              | 8.653                    | 9.086          | 5.542       | 5.973          |
  | **RWKV-6-World-1B6-v2.1**| 1.600       | 8.676                  | 11.240       | 9.546    | 9.595              | 8.842                    | 9.145          | 6.002       | 6.364          |
  | **RWKV-5-World-1B5-v2**      | 1.578       | 8.826                  | 11.391       | 9.741    | 9.795              | 8.972                    | 9.256          | 6.127       | 6.500          |
  | mamba2-1.3b                              | 1.344       | 8.857                  | 11.776       | 9.693    | 10.202             | 8.939                    | 8.864          | 5.899       | 6.623          |
  | mamba-1.4b-hf                            | 1.372       | 8.966                  | 11.845       | 9.770    | 10.320             | 9.082                    | 8.986          | 6.009       | 6.748          |
  | TinyLlama-1.1B-intermediate-step-1431k-3T| 1.100       | 8.988                  | 12.399       | 9.742    | 10.004             | 9.300                    | 9.779          | 5.642       | 6.047          |
  | Qwen1.5-1.8B                             | 1.837       | 9.143                  | 12.452       | 9.894    | 10.609             | 9.138                    | 9.717          | 5.930       | 6.261          |
  | **RWKV-4-World-1.5B-v1**| 1.578       | 9.188                  | 11.465       | 9.943    | 10.109             | 9.573                    | 9.964          | 6.393       | 6.869          |
  | OLMo-1B-hf                               | 1.177       | 9.197                  | 12.137       | 9.605    | 10.342             | 9.259                    | 9.909          | 6.310       | 6.817          |
  | Qwen-1_8B                                | 1.837       | 9.326                  | 12.595       | 9.994    | 10.768             | 9.317                    | 9.960          | 6.164       | 6.483          |
  | pythia-1.4b-v0                           | 1.415       | 9.333                  | 12.323       | 10.129   | 10.780             | 9.320                    | 9.333          | 6.342       | 7.105          |
  | **RWKV-4-Pile-1B5-20220903-8040**           | 1.515       | 9.460                  | 12.214       | 10.162   | 10.884             | 9.594                    | 9.484          | 6.556       | 7.328          |
  | h2o-danube-1.8b-base                     | 1.831       | 9.719                  | 11.647       | 9.184    | 9.515              | 9.338                    | 10.038         | 9.286       | 9.023          |
  | Sheared-LLaMA-1.3B                       | 1.345       | 9.784                  | 12.192       | 9.698    | 10.301             | 9.683                    | 10.382         | 7.893       | 8.339          |
  | bloom-1b7                                 | 1.722       | 9.820                  | 13.430       | 10.917   | 11.413             | 9.490                    | 9.851          | 6.377       | 7.262          |
  | OpenELM-1_1B                             | 1.080       | 10.193                 | 14.926       | 10.581   | 10.518             | 9.490                    | 10.011         | 7.875       | 7.948          |
  | TransNormerLLM-1B                        | 1.020       | 10.448                 | 12.893       | 10.497   | 11.104             | 10.670                   | 11.361         | 7.729       | 8.883          |
  | phi-1_5                                  | 1.418       | 10.498                 | 13.447       | 11.084   | 13.506             | 9.994                    | 11.724         | 7.306       | 6.425          |
  | falcon-rw-1b                             | 1.312       | 12.117                 | 12.062       | 9.561    | 10.490             | 9.631                    | 10.524         | 16.314      | 16.238         |
  </Tabs.Tab>
</Tabs>

## MMLU 测试[#mmlu-eval]

<Callout type="info" emoji="ℹ️">
MMLU 测试（Massive Multitask Language Understanding）是一项用于评估大型语言模型（LLMs）在广泛任务上的多任务语言理解能力的基准测试。

MMLU 涵盖了从初中到研究生水平的57个不同学科，包括数学、物理、历史、法律、生物学等，测试语言模型是否能够在不同领域内进行推理、回答问题和表现出跨学科的知识。
</Callout>

如果使用 lm_eval 的标准格式测试，RWKV-6-World-7B-v2.1 的 MMLU 准确度是 42.8% ：

``` text copy
The following are multiple choice questions (with answers) about abstract algebra.
Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.
A. 0
B. 4
C. 2
D. 6
Answer:
```
如果使用 RWKV 的训练的数据格式作为 prompt，RWKV-6-World-7B-v2.1 的 MMLU 是 46.7%：

``` text copy
User: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.
A. 0
B. 4
C. 2
D. 6

Assistant: The answer is
```
如果使用最适合 RWKV 模型推理的 prompt 模板，RWKV-6-World-7B-v2.1 的 MMLU 是 47.9%：
``` text copy
User: You are a very talented expert in <SUBJECT>. Answer this question:
<Question>
A. <|A|>
B. <|B|>
C. <|C|>
D. <|D|>

Assistant: The answer is
```

<Callout type="info" emoji="ℹ️">
数据来源：https://github.com/Jellyfish042/rwkv_mmlu
</Callout>
