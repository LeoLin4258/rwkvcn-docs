---
title: RWKV微调 - 关于 RWKV
description: 关于 RWKV介绍了RWKV原理架构、如何体验RWKV、基于RWKV的应用、微调RWKV等内容。RWKV是一种具有 GPT 级大型语言模型（LLM）性能的 RNN，也可以像 GPT Transformer 一样直接训练（可并行化）。
keywords: rwkv微调,RWKV微调方法,rwkv全量微调,RWKV LoRA微调,rwkv微调原理
---

import { Callout } from 'nextra/components'
import { Steps } from 'nextra/components'

<Callout type="info" emoji="ℹ️">
如果你对 RWKV 不熟悉，建议先玩玩基础模型，再尝试微调。

很多情况下，用户想要实现的目标可以通过调整提示词（prompts）来完成，比微调要简单得多。
</Callout>


## 全量微调

你可以使用官方仓库的 RWKV 全量微调方法:

- [https://github.com/BlinkDL/RWKV-LM](https://github.com/BlinkDL/RWKV-LM)

具体方法可以查看：[全量微调入门版教程](https://rwkv.cn/RWKV-Fine-Tuning/Full-ft-Simple)。

或者使用 infctx 训练器进行微调：

- [https://github.com/RWKV/RWKV-infctx-trainer/](https://github.com/RWKV/RWKV-infctx-trainer/)

一般来说，使用官方仓库的训练器可以获得更好的性能，使用 infctx 可以牺牲一些速度换取 infctx 大小的支持。

## 其他微调方法

### state tuning 

<Callout type="info" emoji="ℹ️">
RWKV 是纯 RNN，因此可以做 transformer 难以做到的事情。例如，RWKV 作为 RNN 有固定大小的 state，微调 RWKV 的初始 state，就相当于最彻底的 prompt tuning，甚至可以用于 alignment，因为迁移能力很强。

通过微调 RWKV 的 state ，可以使得 RWKV 模型更好地完成某类任务，或遵循某种风格。
</Callout>

state tuning 推荐尝试社区的 [RWKV-PEFT](https://github.com/JL-er/RWKV-PEFT) 项目。

具体微调方法可以查看：RWKV 微调教程 > [State tuning 微调教程](https://rwkv.cn/RWKV-Fine-Tuning/State-Tuning)。

### LoRA 微调

<Callout type="info" emoji="ℹ️">
LORA（Low-Rank Adaptation）是一种针对大型预训练模型的微调技术。它不改变原始模型大部分参数，而是调整模型的部分权重，以此实现对特定任务的优化。
</Callout>

RWKV LoRA 微调方法可以查看：RWKV 微调教程 > [LoRA 微调教程](https://rwkv.cn/RWKV-Fine-Tuning/LoRA-Fine-Tuning)。

---

有关 RWKV 微调的详细文档，请转到[RWKV 微调文档](https://rwkv.cn/RWKV-Fine-Tuning/Introduction)
