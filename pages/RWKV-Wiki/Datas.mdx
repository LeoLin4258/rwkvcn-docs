---
title: RWKV评测等数据 - RWKV百科
description: RWKV百科介绍了RWKV原理架构、如何体验RWKV、基于RWKV的应用、微调RWKV等内容。RWKV是一种具有 GPT 级大型语言模型（LLM）性能的 RNN，也可以像 GPT Transformer 一样直接训练（可并行化）。
keywords: RWKV评测,rwkv跑分,RWKV基准测试,RWKV的benchmark
---

import { Callout } from 'nextra/components'
import { Steps } from 'nextra/components'
import { Cards, Card } from 'nextra/components'



## Uncheatable Eval 测试

<Callout type="info" emoji="ℹ️">
[Uncheatable Eval](https://huggingface.co/spaces/Jellyfish042/UncheatableEval) 是“无法作弊的评测”，它使用最新的论文和新闻文章等实时数据，评估开源大预言模型的真实建模能力和泛化能力。 
</Callout>

以下是 RWKV 和其他模型的 Uncheatable Eval 评分对比：

### 14B 参数模型

| Name                                     | Params (B)  | Average (lower=better) | ao3 ​english | bbc ​news | wikipedia ​english | arxiv ​computer ​science | arxiv ​physics | github ​cpp | github ​python |
|------------------------------------------|-------------|------------------------|--------------|----------|--------------------|--------------------------|----------------|-------------|----------------|
| Mistral-Nemo-Base-2407                   | 12.248      | 7.107                  | 10.070       | 8.081    | 7.954              | 7.419                    | 7.656          | 4.203       | 4.368          |
| **RWKV-x060-World-14B-v2.1-20240719-ctx4096** | 14.069      | 7.609                  | 10.188       | 8.518    | 8.343              | 7.916                    | 8.040          | 4.930       | 5.330          |
| Llama-2-13b-hf                           | 13.016      | 7.676                  | 10.524       | 8.279    | 8.187              | 8.075                    | 8.311          | 4.929       | 5.426          |
| Qwen1.5-14B                              | 14.167      | 7.697                  | 10.880       | 8.884    | 9.102              | 7.752                    | 7.862          | 4.665       | 4.736          |


### 7B 参数模型

| Name                           | Params (B)  | Average (lower=better) | ao3 ​english | bbc ​news | wikipedia ​english | arxiv ​computer ​science | arxiv ​physics | github ​cpp | github ​python |
|---------------------------------|-------------|------------------------|--------------|----------|--------------------|--------------------------|----------------|-------------|----------------|
| Meta-Llama-3.1-8B               | 8.030       | 7.227                  | 10.529       | 8.201    | 7.932              | 7.549                    | 7.686          | 4.204       | 4.487          |
| Qwen2.5-7B                      | 7.616       | 7.453                  | 10.931       | 8.742    | 8.815              | 7.683                    | 7.991          | 3.920       | 4.091          |
| Qwen2-7B                        | 7.616       | 7.528                  | 10.814       | 8.582    | 8.709              | 7.822                    | 8.249          | 4.204       | 4.317          |
| Mistral-7B-v0.1                 | 7.242       | 7.580                  | 10.546       | 8.282    | 8.435              | 7.852                    | 8.092          | 4.800       | 5.051          |
| **RWKV-x060-World-7B-v2.1-20240507-ctx4096** | 7.636 | 7.817                  | 10.410       | 8.740    | 8.577              | 8.107                    | 8.248          | 5.122       | 5.516          |
| Yi-1.5-6B                       | 6.061       | 7.830                  | 10.926       | 8.789    | 8.948              | 8.098                    | 8.410          | 4.746       | 4.890          |
| OLMo-1.7-7B-hf                  | 6.888       | 7.881                  | 11.012       | 8.647    | 8.987              | 8.004                    | 8.200          | 4.966       | 5.354          |
| **RWKV-5-World-7B-v2-20240128-ctx4096**  | 7.518    | 7.905                  | 10.495       | 8.878    | 8.694              | 8.184                    | 8.311          | 5.187       | 5.587          |
| Qwen1.5-7B                      | 7.721       | 7.916                  | 11.100       | 9.126    | 9.357              | 7.953                    | 8.107          | 4.845       | 4.924          |
| mpt-7b                          | 6.649       | 7.950                  | 11.185       | 8.682    | 8.771              | 8.157                    | 8.438          | 4.954       | 5.466          |
| Llama-2-7b-hf                   | 6.738       | 7.967                  | 10.842       | 8.511    | 8.521              | 8.330                    | 8.630          | 5.235       | 5.703          |
| Zamba-7B-v1                     | 7.232       | 8.089                  | 10.846       | 8.520    | 8.639              | 8.060                    | 8.290          | 5.938       | 6.328          |
| open_llama_7b_v2                | 6.738       | 8.103                  | 11.086       | 8.839    | 9.053              | 8.404                    | 8.757          | 4.887       | 5.698          |
| falcon-7b                       | 6.922       | 8.302                  | 10.756       | 8.691    | 9.147              | 8.549                    | 9.060          | 5.763       | 6.149          |
| pythia-6.9b-v0                  | 6.857       | 8.545                  | 11.495       | 9.375    | 9.757              | 8.683                    | 8.572          | 5.611       | 6.321          |
| mamba-7b-rw                     | 6.947       | 9.782                  | 10.808       | 8.548    | 8.990              | 8.613                    | 9.142          | 11.107      | 11.268         |

### 3B 参数模型

| Name                                     | Params (B)  | Average (lower=better) | ao3 ​english | bbc ​news | wikipedia ​english | arxiv ​computer ​science | arxiv ​physics | github ​cpp | github ​python |
|------------------------------------------|-------------|------------------------|--------------|----------|--------------------|--------------------------|----------------|-------------|----------------|
| Qwen1.5-4B                               | 3.950       | 8.250                  | 11.612       | 9.335    | 9.781              | 8.233                    | 8.449          | 5.143       | 5.197          |
| Llama-3.1-Minitron-4B-Depth-Base          | 4.540       | 8.257                  | 11.404       | 9.261    | 9.399              | 8.456                    | 9.068          | 4.912       | 5.298          |
| **RWKV-x060-World-3B-v2.1-20240417-ctx4096** | 3.100       | 8.263                  | 10.835       | 9.156    | 9.103              | 8.492                    | 8.721          | 5.573       | 5.961          |
| Phi-3-mini-4k-instruct                   | 3.821       | 8.333                  | 11.978       | 9.190    | 9.301              | 8.386                    | 9.015          | 5.443       | 5.021          |
| mamba2attn-2.7b                          | 2.698       | 8.359                  | 11.285       | 9.227    | 9.593              | 8.520                    | 8.399          | 5.405       | 6.084          |
| gemma-2b                                 | 2.506       | 8.385                  | 11.741       | 9.137    | 9.391              | 8.686                    | 8.878          | 5.190       | 5.675          |
| **RWKV-5-World-3B-v2-20231113-ctx4096**      | 3.063       | 8.409                  | 10.987       | 9.343    | 9.297              | 8.620                    | 8.836          | 5.699       | 6.080          |
| open_llama_3b_v2                         | 3.426       | 8.459                  | 11.466       | 9.149    | 9.470              | 8.744                    | 9.156          | 5.205       | 6.023          |
| mamba2-2.7b                              | 2.703       | 8.469                  | 11.377       | 9.316    | 9.717              | 8.617                    | 8.489          | 5.524       | 6.243          |
| Phi-3.5-mini-instruct                    | 3.821       | 8.476                  | 12.163       | 9.313    | 9.431              | 8.567                    | 9.147          | 5.513       | 5.197          |
| Zamba2-2.7B                              | 2.689       | 8.571                  | 11.166       | 8.934    | 9.143              | 8.424                    | 8.877          | 6.776       | 6.675          |
| mamba-2.8b-hf                            | 2.768       | 8.593                  | 11.462       | 9.428    | 9.872              | 8.760                    | 8.636          | 5.638       | 6.352          |
| **RWKV-4-World-3B-v1-20230619-ctx4096**      | 3.063       | 8.705                  | 11.042       | 9.512    | 9.588              | 9.129                    | 9.425          | 5.854       | 6.383          |
| pythia-2.8b-v0                           | 2.775       | 8.845                  | 11.815       | 9.684    | 10.155             | 8.923                    | 8.855          | 5.887       | 6.597          |
| RedPajama-INCITE-Base-3B-v1              | 2.776       | 8.869                  | 11.661       | 9.128    | 9.290              | 8.882                    | 9.208          | 6.617       | 7.294          |
| phi-2                                    | 2.780       | 8.911                  | 12.280       | 9.283    | 9.582              | 8.815                    | 9.858          | 6.771       | 5.789          |
| btlm-3b-8k-base                          | 2.646       | 8.959                  | 11.807       | 9.082    | 9.097              | 8.571                    | 8.880          | 7.464       | 7.812          |
| **RWKV-4-Pile-3B-20221110-ctx4096**          | 2.985       | 9.020                  | 11.790       | 9.758    | 10.397             | 9.204                    | 9.059          | 6.085       | 6.850          |
| Sheared-LLaMA-2.7B                       | 2.702       | 9.099                  | 11.580       | 9.146    | 9.613              | 9.107                    | 9.647          | 7.066       | 7.535          |
| MiniCPM3-4B                              | 4.074       | 9.119                  | 13.229       | 10.711   | 10.719             | 8.754                    | 9.034          | 5.545       | 5.839          |
| mamba-2.8b-slimpj                        | 2.768       | 9.247                  | 13.811       | 9.660    | 9.151              | 8.711                    | 8.922          | 7.043       | 7.428          |
| OpenELM-3B                               | 3.037       | 9.679                  | 14.051       | 10.080   | 9.974              | 9.159                    | 9.517          | 7.402       | 7.572          |

### 1.6B 参数模型

| Name                                     | Params (B)  | Average (lower=better) | ao3 ​english | bbc ​news | wikipedia ​english | arxiv ​computer ​science | arxiv ​physics | github ​cpp | github ​python |
|------------------------------------------|-------------|------------------------|--------------|----------|--------------------|--------------------------|----------------|-------------|----------------|
| Qwen2-1.5B                               | 1.544       | 8.457                  | 11.777       | 9.443    | 9.876              | 8.751                    | 9.323          | 5.019       | 5.009          |
| Index-1.9B                               | 2.173       | 8.491                  | 11.662       | 9.495    | 9.521              | 8.546                    | 8.717          | 5.533       | 5.963          |
| stablelm-2-1_6b                          | 1.645       | 8.531                  | 11.563       | 9.211    | 9.381              | 8.791                    | 9.248          | 5.646       | 5.875          |
| Rene-v0.1-1.3b-pytorch                   | 1.330       | 8.559                  | 11.621       | 9.199    | 9.839              | 8.653                    | 9.086          | 5.542       | 5.973          |
| **RWKV-x060-World-1B6-v2.1-20240328-ctx4096**| 1.600       | 8.676                  | 11.240       | 9.546    | 9.595              | 8.842                    | 9.145          | 6.002       | 6.364          |
| **RWKV-5-World-1B5-v2-20231025-ctx4096**      | 1.578       | 8.826                  | 11.391       | 9.741    | 9.795              | 8.972                    | 9.256          | 6.127       | 6.500          |
| mamba2-1.3b                              | 1.344       | 8.857                  | 11.776       | 9.693    | 10.202             | 8.939                    | 8.864          | 5.899       | 6.623          |
| mamba-1.4b-hf                            | 1.372       | 8.966                  | 11.845       | 9.770    | 10.320             | 9.082                    | 8.986          | 6.009       | 6.748          |
| TinyLlama-1.1B-intermediate-step-1431k-3T| 1.100       | 8.988                  | 12.399       | 9.742    | 10.004             | 9.300                    | 9.779          | 5.642       | 6.047          |
| Qwen1.5-1.8B                             | 1.837       | 9.143                  | 12.452       | 9.894    | 10.609             | 9.138                    | 9.717          | 5.930       | 6.261          |
| **RWKV-4-World-1.5B-v1-fixed-20230612-ctx4096**| 1.578       | 9.188                  | 11.465       | 9.943    | 10.109             | 9.573                    | 9.964          | 6.393       | 6.869          |
| OLMo-1B-hf                               | 1.177       | 9.197                  | 12.137       | 9.605    | 10.342             | 9.259                    | 9.909          | 6.310       | 6.817          |
| Qwen-1_8B                                | 1.837       | 9.326                  | 12.595       | 9.994    | 10.768             | 9.317                    | 9.960          | 6.164       | 6.483          |
| pythia-1.4b-v0                           | 1.415       | 9.333                  | 12.323       | 10.129   | 10.780             | 9.320                    | 9.333          | 6.342       | 7.105          |
| **RWKV-4-Pile-1B5-20220903-8040**           | 1.515       | 9.460                  | 12.214       | 10.162   | 10.884             | 9.594                    | 9.484          | 6.556       | 7.328          |
| h2o-danube-1.8b-base                     | 1.831       | 9.719                  | 11.647       | 9.184    | 9.515              | 9.338                    | 10.038         | 9.286       | 9.023          |
| Sheared-LLaMA-1.3B                       | 1.345       | 9.784                  | 12.192       | 9.698    | 10.301             | 9.683                    | 10.382         | 7.893       | 8.339          |
| bloom-1b7                                 | 1.722       | 9.820                  | 13.430       | 10.917   | 11.413             | 9.490                    | 9.851          | 6.377       | 7.262          |
| OpenELM-1_1B                             | 1.080       | 10.193                 | 14.926       | 10.581   | 10.518             | 9.490                    | 10.011         | 7.875       | 7.948          |
| TransNormerLLM-1B                        | 1.020       | 10.448                 | 12.893       | 10.497   | 11.104             | 10.670                   | 11.361         | 7.729       | 8.883          |
| phi-1_5                                  | 1.418       | 10.498                 | 13.447       | 11.084   | 13.506             | 9.994                    | 11.724         | 7.306       | 6.425          |
| falcon-rw-1b                             | 1.312       | 12.117                 | 12.062       | 9.561    | 10.490             | 9.631                    | 10.524         | 16.314      | 16.238         |


## MMLU 测试

<Callout type="info" emoji="ℹ️">
MMLU 测试（Massive Multitask Language Understanding）是一项用于评估大型语言模型（LLMs）在广泛任务上的多任务语言理解能力的基准测试。

MMLU 涵盖了从初中到研究生水平的57个不同学科，包括数学、物理、历史、法律、生物学等，测试语言模型是否能够在不同领域内进行推理、回答问题和表现出跨学科的知识。
</Callout>

如果使用 lm_eval 的标准格式测试，RWKV-6-7B-v2.1 的 MMLU 准确度是 42.8% ：

``` text copy
The following are multiple choice questions (with answers) about abstract algebra.
Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.
A. 0
B. 4
C. 2
D. 6
Answer:
```
如果使用 RWKV 的标准 prompt 格式测试，RWKV-6-7B-v2.1 的 MMLU是 46.7%：

``` text copy
User: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q.
A. 0
B. 4
C. 2
D. 6

Assistant: The answer is
```


<Callout type="info" emoji="ℹ️">
数据来源：https://github.com/Jellyfish042/rwkv_mmlu
</Callout>

## RULER 测试
<Callout type="info" emoji="ℹ️">
[RULER](https://arxiv.org/abs/2404.06654) 测试是一个新的 LLM 测试方法，相比于 NIAH 大海捞针测试做了优化和扩展，其包含四种测试任务：检索（NIAH 扩展版本）、多跳跟踪（Multi-hop Tracing）、信息聚合（CWE、FWE）、带干扰的问答（QA）。

1. **增强的大海捞针（NIAH）**：该任务扩展了 NIAH，即大海捞针测试，分为四个子任务评估不同的检索能力：Single NIAH (S-NIAH)、Multi-keys NIAH (MK-NIAH)、Multi-values NIAH (MV-NIAH)、Multi-queries NIAH (MQ-NIAH)；
2. **多跳跟踪 - 变量跟踪（Multi-hop Tracing: Variable Tracking）**：该任务主要检查模型能否在长上下文中成功识别并跟踪具有多跳连接的实体（变量）和指代关系。比如赋值 $X_1 = V $，然后 $X_2 = X_1$、$X_3 = X_2$ ... 最终返回所有指向值 $V$ 的变量名；
3. **信息聚合（CWE、FWE）**：该任务为常用词（Common Words）/高频词（Frequent Words）提取，用于测试模型**跨长上下文聚合常见信息**的能力；
4. **问答（QA）**：该任务在现有短上下文问答数据集的输入中添加了干扰信息，以评估各种上下文大小下的问答能力。
</Callout>

RWKV-6-7B-v2.1 模型在 RULER 测试的各项得分如下表：

| 任务名称 | 得分 |
| --- | --- |
| NIAH_single_1 | 100 |
| NIAH_single_2 | 98.67 |
| NIAH_single_3 | 95 |
| NIAH_multikey_1 | 48.33 |
| NIAH_multikey_2 | 7.67 |
| NIAH_multikey_3 | 1.33 |
| NIAH_multivalue | 80.42 |
| NIAH_multiquery | 83.67 |
| 多跳跟踪 | 7.53 |
| 常用词提取 （CWE） | 38.6 |
| 高频词提取 （FWE） | 78.33 |
| qa_1 | 45 |
| qa_2 | 37 |

<Callout type="info" emoji="ℹ️">
数据来源：[https://github.com/Ojiyumm/RULER_RWKV](https://github.com/Ojiyumm/RULER_RWKV)
</Callout>


## LongBench 测试

<Callout type="info" emoji="ℹ️">
[LongBench](https://arxiv.org/abs/2308.14508) 测试是一个针对大语言模型长文本理解能力的评测基准。

LongBench 共有六大类、二十一个不同的中英双语任务，覆盖了单文档 QA、多文档 QA、摘要、Few-shot 学习、合成任务和代码补全等关键的长文本应用场景。
</Callout>

以下是 RWKV-6-7B-v2.1 和其他模型的 LongBench 测试分数对比，数据表格按照六个分类展示：

### Single-Document QA （单文档问答）

单文档问答包含以下四种测试任务：

| 任务 | 任务说明 |
| --- | --- |
| NarrativeQA | 基于故事或剧本提问，包括对人物、情节、主题等重要元素的理解 |
| Qasper | 基于单篇论文的提出，问题由 NLP 的读者提出，并由 NLP 从业者回答 |
| MultiFieldQA-en | 基于单篇文档回答英文问题，文档所属的领域相对多元 |
| MultiFieldQA-zh | 基于单篇文档回答中文问题，文档所属的领域相对多元 |

**Single-Document QA 测试结果：**

| 模型 | NarrativeQA | Qasper | MultiFieldQA-en | MultiFieldQA-zh |
| --- | --- | --- | --- | --- |
| GPT-3.5-Turbo-16k | 23.6 | 43.3 | 52.3 | 61.2 |
| Llama2-7B-chat-4k | 18.7 | 19.2 | 36.8 | 11.9 |
| LongChat-v1.5-7B-32k | 16.9 | 27.7 | 41.4 | 29.1 |
| XGen-7B-8k | 18.0 | 18.1 | 37.7 | 14.8 |
| InternLM-7B-8k | 12.1 | 16.7 | 23.4 | 33.6 |
| ChatGLM2-6B-32k | 21.1 | 31.5 | 46.2 | 51.6 |
| Vicuna-v1.5-7B-16k | 19.4 | 26.1 | 38.5 | 43.0 |
| ChatGLM3-6B-32k | 26.0 | 43.3 | 51.7 | 62.3 |
| Mamba_1B4 | 2.23  | 4.44  | 11.33  | 13.03  |
| Mamba_2B8 | 2.32  | 4.89  | 8.15  | 6.83  |
| Llma2_7B | 18.7  | 19.2  | 11.90  | 36.8  |
| mistral_7B | 12.79 | 8.9 | 30.55 | 17.91 |
| **RWKV-6-v2.1-1B6** | 4.53  | 19.79  | 22.99  | 18.57  |
| **RWKV-6-v2.1-3B** | 2.87  | 14.2  | 18.78  | 21.49  |
| **RWKV-6-v2.1-7b-4k** | 20.75  | 40.2  | 36.01  | 50.19  |

### Multi-Document QA（多文档问答）

多文档问答包含以下四种测试任务：

| 任务 | 任务说明 |
| --- | --- |
| HotpotQA | 基于 HotpotQA 文档回答问题，HotpotQA 涉及许多由母语人士根据两个相关段落编写的 2 跳问题 |
| 2WikiMultihopQA | 基于 2WikiMultihopQA 数据回答问题，2WikiMultihopQA 由最多 5 跳问题组成，这些问题通过手动设计的模板合成 |
| MuSiQue | 基于 MuSiQue 数据回答问题，MuSiQue 由最多 4 跳推理（4-hop reasoning）的简单问题组合而成 |
| DuReader | 基于 DuReader 的中文数据集回答相关问题，包含来自基于百度搜索和百度知道的 20 万个问题和 1M 文档 |

**Multi-Document QA 测试结果：**

| 模型 | HotpotQA | 2WikiMQA | Musique | DuReader (zh) |
| --- | --- | --- | --- | --- |
| GPT-3.5-Turbo-16k | 51.6 | 37.7 | 26.9 | 28.7 |
| Llama2-7B-chat-4k | 25.4 | 32.8 | 9.4 | 5.2 |
| LongChat-v1.5-7B-32k | 31.5 | 20.6 | 9.7 | 19.5 |
| XGen-7B-8k | 29.7 | 21.1 | 10.3 | 11.0 |
| InternLM-7B-8k | 28.7 | 22.8 | 9.0 | 11.1 |
| ChatGLM2-6B-32k | 45.1 | 34.0 | 21.9 | 37.6 |
| Vicuna-v1.5-7B-16k | 25.3 | 20.8 | 9.8 | 19.3 |
| ChatGLM3-6B-32k | 54.4 | 44.9 | 40.4 | 44.78 |
| Mamba_1B4 | 5.73  | 8.77  | 3.3  | 11.95  |
| Mamba_2B8 | 5.49  | 8.45  | 3.45  | 13.96  |
| Llma2_7B | 25.4  | 32.8  | 9.4  | 5.2  |
| mistral_7B | 9.39 | 11.17 | 4.58 | 11.68 |
| **RWKV-6-v2.1-1B6** | 8.72  | 11.86  | 3.96  | 11.40  |
| **RWKV-6-v2.1-3B** | 6.79  | 9.64  | 4.13  | 17.41  |
| **RWKV-6-v2.1-7b-4k** | 22.74  | 16.3  | 10.49  | 28.01  |


### Summarization（摘要）

摘要任务涉及以下四种测试：

| 任务 | 任务说明 |
| --- | --- |
| GovReport | 摘要任务，要求对政府的工作报告进行总结摘要 |
| QMSum | 摘要任务，要求基于用户的查询对会议记录进行摘要 |
| MultiNews | 多文档摘要任务，要求基于多篇新闻进行摘要 |
| VCSUM | 摘要任务，要求对中文会议记录进行总结摘要 |

**摘要任务测试结果如下：**

| 模型 | GovReport | QMSum | MultiNews | VCSUM (zh) |
| --- | --- | --- | --- | --- |
| GPT-3.5-Turbo-16k | 29.5 | 23.4 | 26.7 | 16.0 |
| Llama2-7B-chat-4k | 27.3 | 20.8 | 25.8 | 0.2 |
| LongChat-v1.5-7B-32k | 30.8 | 22.7 | 26.4 | 9.9 |
| XGen-7B-8k | 27.3 | 20.5 | 26.2 | 2.2 |
| InternLM-7B-8k | 9.7 | 15.9 | 22.8 | 12.4 |
| ChatGLM2-6B-32k | 32.4 | 24.0 | 26.5 | 16.2 |
| Vicuna-v1.5-7B-16k | 27.9 | 22.8 | 27.2 | 15.1 |
| ChatGLM3-6B-32k | 36.8 | 23.9 | 27.9 | 17.8 |
| Mamba_1B4 | 9.34  | 10.85  | 15.86  | 6.33  |
| Mamba_2B8 | 10.41  | 11.42  | 18.94  | 6.1  |
| Llma2_7B | 27.3  | 20.8  | 25.8  | 0.2  |
| mistral_7B | 28.84 | 20.32 | 22.79 | 4.1 |
| **RWKV-6-v2.1-1B6** | 17.51  | 20.36  | 21.52  | 10.71  |
| **RWKV-6-v2.1-3B** | 19.21  | 21  | 21.76  | 10.18  |
| **RWKV-6-v2.1-7b-4k** | 31.64  | 21.31  | 26.06  | 15.19  |
### Few-shot Learning（小样本学习）

小样本学习包含以下四种测试任务：

| 任务 | 任务说明 |
| --- | --- |
| TREC | 分类任务，要求对问题进行分类，一共包含 50 个类别 |
| TriviaQA | 单文档问答任务，提供若干的 Few Shot 样例 |
| SAMSum | 对话摘要任务，提供若干的 Few Shot 样例 |
| LSHT | 中文分类任务，要求对新闻进行分类，一共包含 24 个类别 |

**Few-shot Learning测试结果如下：**

| 模型 | TREC | TriviaQA | SAMSum | LSHT (zh) |
| --- | --- | --- | --- | --- |
| GPT-3.5-Turbo-16k | 68.0 | 91.4 | 41.7 | 29.2 |
| Llama2-7B-chat-4k | 61.5 | 77.8 | 40.7 | 19.8 |
| LongChat-v1.5-7B-32k | 63.5 | 82.3 | 34.2 | 23.2 |
| XGen-7B-8k | 65.5 | 77.8 | 25.3 | 20.5 |
| InternLM-7B-8k | 52.0 | 77.8 | 21.2 | 15.2 |
| ChatGLM2-6B-32k | 62.5 | 78.7 | 36.3 | 27.7 |
| Vicuna-v1.5-7B-16k | 71.5 | 86.2 | 40.8 | 28.8 |
| ChatGLM3-6B-32k | 79.0 | 87.1 | 38.2 | 42.0 |
| Mamba_1B4 | 45.5  | 37.33  | 12.56  | 8.5  |
| Mamba_2B8 | 21.5  | 34.62  | 9.3  | 5  |
| Llma2_7B | 61.5  | 77.8  | 40.7  | 19.8  |
| mistral_7B | 70.0 | 89.26 | 43.74 | 25.5 |
| **RWKV-6-v2.1-1B6** | 39.5  | 47.64  | 13.58  | 18.8  |
| **RWKV-6-v2.1-3B** | 51.5  | 57.15  | 17.95  | 15.2  |
| **RWKV-6-v2.1-7b-4k** | 55.5  | 86.89  | 44.25  | 30.2  |

### Synthetic Tasks（合成任务）

合成任务测试任务包含以下三种测试任务：

| 任务 | 任务说明 |
| --- | --- |
| PassageCount | 判断给定的若干的段落中不重复的段落一共有几个 |
| PassageRetrieval-en | 给定 30 个英文维基的段落，判断给定的摘要属于哪个段落 |
| PassageRetrieval-zh | 给定若干个出自 C4 数据集的中文段落，判断给定的摘要属于哪个段落 |

**Synthetic Tasks 测试结果如下：**

| 模型 | Passage Count | PassageRetrieval-en | PassageRetrieval-zh |
| --- | --- | --- | --- |
| GPT-3.5-Turbo-16k | 4.5 | 71.0 | 77.5 |
| Llama2-7B-chat-4k | 2.1 | 9.8 | 0.5 |
| LongChat-v1.5-7B-32k | 1.0 | 30.5 | 7.6 |
| XGen-7B-8k | 2.1 | 8.5 | 3.5 |
| InternLM-7B-8k | 3.0 | 6.0 | 0.9 |
| ChatGLM2-6B-32k | 1.5 | 77.0 | 64.5 |
| Vicuna-v1.5-7B-16k | 6.5 | 4.5 | 5.0 |
| ChatGLM3-6B-32k | 2.0 | 99.0 | 94.0 |
| Mamba_1B4 | 0.45  | 3.32  | 3.81  |
| Mamba_2B8 | 0.74  | 1.83  | 3.37  |
| Llma2_7B | 2.1  | 9.8  | 0.5  |
| mistral_7B | 1.05 | 12.5 | 16.75 |
| **RWKV-6-v2.1-1B6** | 0  | 4.25  | 4.16  |
| **RWKV-6-v2.1-3B** | 0  | 3.83  | 4.12  |
| **RWKV-6-v2.1-7b-4k** | 5  | 34.5  | 54.22  |

### Code Completion（代码续写）

代码续写包含以下两种测试任务：

| 任务 | 任务说明 |
| --- | --- |
| LCC | 给定一段较长代码，要求预测出下一行代码 |
| RepoBench-P | 给定一个 github 仓库内多个文件中的代码（包含文件间依赖），要求预测出下一行代码 |

**代码续写测试结果如下：**

| 模型 | LCC | RepoBench-P |
| --- | --- | --- |
| GPT-3.5-Turbo-16k | 54.7 | 53.6 |
| Llama2-7B-chat-4k | 52.4 | 43.8 |
| LongChat-v1.5-7B-32k | 53.0 | 55.3 |
| XGen-7B-8k | 38.6 | 38.6 |
| InternLM-7B-8k | 44.1 | 28.8 |
| ChatGLM2-6B-32k | 55.6 | 49.9 |
| Vicuna-v1.5-7B-16k | 51.0 | 43.5 |
| ChatGLM3-6B-32k | 57.66 | 54.76 |
| Mamba_1B4 | 44.33  | 41.86  |
| Mamba_2B8 | 39.53  | 24.38  |
| Llma2_7B | 52.4  | 43.8  |
| mistral_7B | 70.64 | 59.7 |
| **RWKV-6-v2.1-1B6** | 39.5  | 40.44  |
| **RWKV-6-v2.1-3B** | 40.01  | 41.35  |
| **RWKV-6-v2.1-7b-4k** | 73.84  | 54.1  |

### RWKV、Mamba 和 Llma2 的综合评分对比

| 模型 | Single DocQ | Few-shc | Summarization | Multi Doc QA | Code Completion | Syntetic |
| --- | --- | --- | --- | --- | --- | --- |
| **RWKV-6-v2.1-1B6** | 16.470  | 29.868  | 17.525  | 8.985  | 39.970  | 2.803  |
| **RWKV-6-v2.1-3B** | 14.335  | 35.443  | 18.038  | 9.493  | 40.680  | 2.650  |
| **RWKV-6-v2.1-7b-4k** | 36.788  | 54.203  | 23.550  | 19.385  | 63.970  | 31.240  |
| Mamba_1B4 | 7.758  | 25.973  | 10.595  | 7.438  | 43.095  | 2.527  |
| Mamba_2B8 | 5.548  | 17.605  | 11.718  | 7.838  | 31.955  | 1.980  |
| Llma2_7B | 21.650  | 49.950  | 18.525  | 18.200  | 48.100  | 4.133  |
| mistral_7B | 17.538 | 52.833 | 19.013 | 9.205 | 65.17 | 10.100 |


<Callout type="info" emoji="ℹ️">
评测数据来源：[https://github.com/Ojiyumm/LongBench_RWKV](https://github.com/Ojiyumm/LongBench_RWKV)
</Callout>


